(window.webpackJsonp=window.webpackJsonp||[]).push([[68],{391:function(t,a,s){"use strict";s.r(a);var r=s(18),_=Object(r.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"lora-быстрая-адаптация-моделеи"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lora-быстрая-адаптация-моделеи"}},[t._v("#")]),t._v(" LoRA: быстрая адаптация моделей")]),t._v(" "),a("p",[a("strong",[t._v("LoRA")]),t._v(" (Low-Rank Adaptation) — это техника эффективного дообучения больших моделей, которая позволяет адаптировать модель под свои задачи, обучая только небольшую часть параметров вместо всей модели.")]),t._v(" "),a("h2",{attrs:{id:"проблема-которую-решает-lora"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#проблема-которую-решает-lora"}},[t._v("#")]),t._v(" Проблема, которую решает LoRA")]),t._v(" "),a("p",[t._v("Обычный fine-tuning требует:")]),t._v(" "),a("ul",[a("li",[t._v("Много вычислительных ресурсов (GPU, время)")]),t._v(" "),a("li",[t._v("Много памяти для хранения всех весов модели")]),t._v(" "),a("li",[t._v("Долгое время обучения")])]),t._v(" "),a("p",[t._v('LoRA решает это, обучая только небольшие дополнительные матрицы, которые "адаптируют" оригинальную модель.')]),t._v(" "),a("h2",{attrs:{id:"как-работает-lora"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#как-работает-lora"}},[t._v("#")]),t._v(" Как работает LoRA?")]),t._v(" "),a("p",[t._v("Вместо изменения всех весов модели, LoRA:")]),t._v(" "),a("ol",[a("li",[t._v("Добавляет небольшие матрицы (low-rank matrices) к некоторым слоям")]),t._v(" "),a("li",[t._v("Обучает только эти новые матрицы")]),t._v(" "),a("li",[t._v("При использовании комбинирует оригинальные веса с обученными матрицами")])]),t._v(" "),a("p",[t._v("Результат: модель работает как дообученная, но:")]),t._v(" "),a("ul",[a("li",[t._v("Обучается в 10-100 раз быстрее")]),t._v(" "),a("li",[t._v("Требует в 10-100 раз меньше памяти")]),t._v(" "),a("li",[t._v("Можно хранить много разных адаптаций одной модели")])]),t._v(" "),a("h2",{attrs:{id:"преимущества-lora"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#преимущества-lora"}},[t._v("#")]),t._v(" Преимущества LoRA")]),t._v(" "),a("p",[t._v("✅ "),a("strong",[t._v("Быстрое обучение")]),t._v(": часы вместо дней\n✅ "),a("strong",[t._v("Экономия памяти")]),t._v(": можно обучать на обычных GPU\n✅ "),a("strong",[t._v("Модульность")]),t._v(": можно переключаться между разными адаптациями\n✅ "),a("strong",[t._v("Эффективность")]),t._v(": результаты близки к полному fine-tuning")]),t._v(" "),a("h2",{attrs:{id:"когда-использовать-lora"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#когда-использовать-lora"}},[t._v("#")]),t._v(" Когда использовать LoRA?")]),t._v(" "),a("p",[t._v("✅ Нужно быстро протестировать разные адаптации\n✅ Ограниченные вычислительные ресурсы\n✅ Нужно несколько специализированных версий модели\n✅ Экспериментирование с разными задачами")]),t._v(" "),a("p",[t._v("❌ Нужна максимальная точность (полный fine-tuning может быть лучше)\n❌ Очень специфические задачи, требующие глубоких изменений")]),t._v(" "),a("h2",{attrs:{id:"технические-детали"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#технические-детали"}},[t._v("#")]),t._v(" Технические детали")]),t._v(" "),a("h3",{attrs:{id:"rank-ранг"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rank-ранг"}},[t._v("#")]),t._v(" Rank (ранг)")]),t._v(" "),a("p",[t._v("Параметр, который определяет размер адаптационных матриц:")]),t._v(" "),a("ul",[a("li",[t._v("Низкий rank (1-4): быстрее, меньше памяти, но может быть менее точным")]),t._v(" "),a("li",[t._v("Высокий rank (16-64): медленнее, больше памяти, но точнее")])]),t._v(" "),a("h3",{attrs:{id:"alpha"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#alpha"}},[t._v("#")]),t._v(" Alpha")]),t._v(" "),a("p",[t._v("Параметр масштабирования, влияет на силу адаптации.")]),t._v(" "),a("h3",{attrs:{id:"target-modules"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#target-modules"}},[t._v("#")]),t._v(" Target modules")]),t._v(" "),a("p",[t._v("Выбор слоёв для адаптации (обычно attention layers).")]),t._v(" "),a("h2",{attrs:{id:"популярные-библиотеки"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#популярные-библиотеки"}},[t._v("#")]),t._v(" Популярные библиотеки")]),t._v(" "),a("h3",{attrs:{id:"peft-hugging-face"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#peft-hugging-face"}},[t._v("#")]),t._v(" PEFT (Hugging Face)")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" peft "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" get_peft_model\n\nconfig "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LoraConfig"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    r"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# rank")]),t._v("\n    lora_alpha"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    target_modules"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"q_proj"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"v_proj"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    lora_dropout"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_peft_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("base_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("h3",{attrs:{id:"lora-для-разных-моделеи"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lora-для-разных-моделеи"}},[t._v("#")]),t._v(" LoRA для разных моделей")]),t._v(" "),a("ul",[a("li",[t._v("Llama, Mistral: хорошо работают с LoRA")]),t._v(" "),a("li",[t._v("GPT-модели: можно использовать через специальные библиотеки")]),t._v(" "),a("li",[t._v("Stable Diffusion: популярно для генерации изображений")])]),t._v(" "),a("h2",{attrs:{id:"пример-workflow"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#пример-workflow"}},[t._v("#")]),t._v(" Пример workflow")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("Выбор базовой модели")]),t._v(": например, Llama 2 7B")]),t._v(" "),a("li",[a("strong",[t._v("Подготовка данных")]),t._v(": специфические примеры для задачи")]),t._v(" "),a("li",[a("strong",[t._v("Настройка LoRA")]),t._v(": выбор rank, alpha, target modules")]),t._v(" "),a("li",[a("strong",[t._v("Обучение")]),t._v(": обычно несколько часов на одной GPU")]),t._v(" "),a("li",[a("strong",[t._v("Тестирование")]),t._v(": проверка на валидационных данных")]),t._v(" "),a("li",[a("strong",[t._v("Использование")]),t._v(": загрузка базовой модели + LoRA веса")])]),t._v(" "),a("h2",{attrs:{id:"сравнение-с-fine-tuning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#сравнение-с-fine-tuning"}},[t._v("#")]),t._v(" Сравнение с fine-tuning")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Параметр")]),t._v(" "),a("th",[t._v("Fine-tuning")]),t._v(" "),a("th",[t._v("LoRA")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("Скорость обучения")]),t._v(" "),a("td",[t._v("Дни")]),t._v(" "),a("td",[t._v("Часы")])]),t._v(" "),a("tr",[a("td",[t._v("Память")]),t._v(" "),a("td",[t._v("Вся модель")]),t._v(" "),a("td",[t._v("Небольшие матрицы")])]),t._v(" "),a("tr",[a("td",[t._v("Точность")]),t._v(" "),a("td",[t._v("Высокая")]),t._v(" "),a("td",[t._v("Очень близкая")])]),t._v(" "),a("tr",[a("td",[t._v("Гибкость")]),t._v(" "),a("td",[t._v("Одна версия")]),t._v(" "),a("td",[t._v("Много адаптаций")])])])]),t._v(" "),a("h2",{attrs:{id:"ограничения"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ограничения"}},[t._v("#")]),t._v(" Ограничения")]),t._v(" "),a("ul",[a("li",[t._v("Может быть немного менее точным, чем полный fine-tuning")]),t._v(" "),a("li",[t._v("Нужно понимать параметры (rank, alpha)")]),t._v(" "),a("li",[t._v("Не все модели одинаково хорошо работают с LoRA")])]),t._v(" "),a("h2",{attrs:{id:"уровень"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#уровень"}},[t._v("#")]),t._v(" Уровень")]),t._v(" "),a("p",[t._v("3")]),t._v(" "),a("h2",{attrs:{id:"примеры-из-жизни"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#примеры-из-жизни"}},[t._v("#")]),t._v(" Примеры из жизни")]),t._v(" "),a("ul",[a("li",[t._v("Быстрое создание специализированных версий модели для разных задач")]),t._v(" "),a("li",[t._v("Адаптация модели под стиль компании")]),t._v(" "),a("li",[t._v("Экспериментирование с разными доменами (медицина, право, код)")]),t._v(" "),a("li",[t._v("Создание персонализированных моделей для разных пользователей")])]),t._v(" "),a("h2",{attrs:{id:"полезные-ссылки-для-этого-уровня"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#полезные-ссылки-для-этого-уровня"}},[t._v("#")]),t._v(" Полезные ссылки для этого уровня")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://huggingface.co/docs/peft",target:"_blank",rel:"noopener noreferrer"}},[t._v("PEFT Documentation"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://arxiv.org/abs/2106.09685",target:"_blank",rel:"noopener noreferrer"}},[t._v("LoRA Paper"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://github.com/bmaltais/kohya_ss",target:"_blank",rel:"noopener noreferrer"}},[t._v("LoRA для Stable Diffusion"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"заметки"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#заметки"}},[t._v("#")]),t._v(" Заметки")])])}),[],!1,null,null,null);a.default=_.exports}}]);
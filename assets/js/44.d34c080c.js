(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{368:function(t,v,_){"use strict";_.r(v);var s=_(18),a=Object(s.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"почему-ии-иногда-ошибается-полное-руководство-для-новичков"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#почему-ии-иногда-ошибается-полное-руководство-для-новичков"}},[t._v("#")]),t._v(" Почему ИИ иногда ошибается? Полное руководство для новичков")]),t._v(" "),v("p",[v("strong",[t._v("Краткая суть:")]),t._v(" Искусственный интеллект ошибается не потому, что сломан или неправильно программирован, а потому что так устроена природа машинного обучения. ИИ учится на примерах, и если примеры неполные, противоречивые или смещённые, модель усваивает эти ошибки. Кроме того, ИИ не понимает мир как человек — он работает с вероятностями и статистикой, что иногда приводит к уверенным, но неправильным ответам. На уровне 0 вы узнаете, почему это происходит, какие типы ошибок встречаются чаще всего, и что вы можете сделать, чтобы защитить себя от этих ошибок.")]),t._v(" "),v("h2",{attrs:{id:"парадокс-мощного-искусственного-интеллекта"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#парадокс-мощного-искусственного-интеллекта"}},[t._v("#")]),t._v(" Парадокс мощного искусственного интеллекта")]),t._v(" "),v("p",[t._v("Когда вы впервые работаете с ИИ, он кажется поразительно умным. ChatGPT может написать стихотворение, решить математическую задачу, дать совет по написанию письма. Но затем вы замечаете что-то странное: ИИ говорит вам сделать что-то опасное, как будто это совершенно обычная рекомендация. Или он увеличивает на 50 — это совсем не то, что вы просили[5]. Или он уверенно рассказывает вам о событии, которого никогда не было[7].")]),t._v(" "),v("p",[t._v("Это создаёт интересный парадокс: система, которая может генерировать текст, звучащий как настоящий человек, может одновременно давать совершенно неправильную информацию с полной уверенностью. Система, которая прошла тысячи тестов и показала высокие результаты, может вдруг на простом примере дать неправильный ответ[4][5].")]),t._v(" "),v("p",[t._v("Это не является признаком того, что ИИ неправильно функционирует. Это просто показывает, что ошибки встроены в саму структуру того, как работает машинное обучение. Чтобы понять, почему это происходит, нужно вернуться к основам: как ИИ на самом деле учится и почему этот процесс неизбежно приводит к ошибкам[1][2].")]),t._v(" "),v("h2",{attrs:{id:"основная-парадигма-ии-учится-на-примерах-а-не-на-правилах"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#основная-парадигма-ии-учится-на-примерах-а-не-на-правилах"}},[t._v("#")]),t._v(" Основная парадигма: ИИ учится на примерах, а не на правилах")]),t._v(" "),v("p",[t._v("Вспомните, как мы обсуждали обучение ИИ на примере распознавания котов. Система не получает явное правило «кот — это животное с четырьмя лапами и острыми ушами». Вместо этого ей показывают тысячи фотографий котов и учат распознавать закономерности в пиксельных данных[4][6].")]),t._v(" "),v("p",[t._v("Эта парадигма «обучение на примерах» имеет фундаментальное преимущество и фундаментальный недостаток. Преимущество в том, что ИИ может работать с очень сложными задачами, которые было бы невозможно описать через явные правила. Недостаток в том, что ИИ может усваивать только то, что видит в примерах, и если примеры неправильные или неполные, то и ИИ обучится неправильно[2][5].")]),t._v(" "),v("p",[t._v("Вот конкретный пример: если вы хотите обучить ИИ распознавать, является ли человек экстровертом на основе его социальных медиа, вы даёте системе примеры — посты, фотографии, комментарии людей, которых вы классифицировали как экстровертов или интровертов. Но здесь возникает первая проблема: что считается экстравертом? Разные люди дадут разные ответы. Один человек может посчитать кого-то экстравертом, другой — нет. Система усредняет эти противоречивые сигналы и начинает воспроизводить эту неясность[2].")]),t._v(" "),v("p",[t._v("Вторая проблема ещё глубже. Люди часто демонстрируют разное поведение в разных контекстах. Человек может быть тихим дома, но жизнерадостным на вечеринках. ИИ, если он обучен только на данных из социальных сетей, может вообще не видеть домашнего поведения и сделать неправильные выводы[5][6].")]),t._v(" "),v("p",[t._v("Вот почему первый источник ошибок в ИИ — это "),v("strong",[t._v("качество и полнота тренировочных данных")]),t._v(". Если данные плохие, модель будет плохой. Если данные неполные, модель будет неполной[2][5].")]),t._v(" "),v("h2",{attrs:{id:"галлюцинации-когда-ии-выдумывает-информацию"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#галлюцинации-когда-ии-выдумывает-информацию"}},[t._v("#")]),t._v(" Галлюцинации: когда ИИ выдумывает информацию")]),t._v(" "),v("p",[t._v("Одна из самых странных и беспокойных ошибок, которые может совершить ИИ, — это галлюцинация. Этот термин означает, что ИИ с полной уверенностью говорит что-то, что не соответствует действительности и чего он точно не знает[4][7][10].")]),t._v(" "),v("p",[t._v('Классический пример: вы спрашиваете ChatGPT «Какие фильмы снял режиссёр Иван Петров?» И ИИ отвечает: «Иван Петров снял такие фильмы как "Дорога домой" (2018), "Ночная полёта" (2021) и "Восход" (2023)». Ответ звучит авторитетно, включает конкретные названия и годы. Но режиссёра Ивана Петрова не существует, и эти фильмы никогда не были сняты. ИИ просто вывел, что это звучит как правдоподобный ответ на основе закономерностей, которые он видел в тренировочных данных[4][7].')]),t._v(" "),v("p",[t._v("Почему это происходит? Важно вернуться к тому, как ИИ на самом деле работает. Большие языковые модели, как ChatGPT, по сути являются очень мощными предсказателями следующего слова[4][10]. Когда вы вводите запрос, система не «вспоминает» ответ из базы данных. Она генерирует ответ слово за словом, каждый раз выбирая наиболее вероятное следующее слово на основе контекста[4].")]),t._v(" "),v("p",[t._v("Это создаёт проблему. Система оптимизирована для того, чтобы выдать ответ, который звучит правдоподобно и логично, а не ответ, который гарантировано правилен[7][10]. Если ИИ не знает ответ на вопрос, он не скажет «я не знаю». Вместо этого он будет генерировать слова, которые, по его статистическому анализу, должны идти дальше. Результат выглядит убедительно, но может быть совершенно неправдиво[7][10].")]),t._v(" "),v("p",[t._v("Исследование компании Vectara показало, что разные ИИ галлюцинируют с поразительно разными частотами. Одни модели выдумывают информацию в 3-5% случаев, другие — в 27% случаев[7][10]. Даже более новые и мощные модели, которые должны быть лучше, иногда галлюцинируют чаще, чем их предшественники. Модель o3 от OpenAI галлюцинировала в 51% случаев при ответе на простые фактические вопросы[48].")]),t._v(" "),v("p",[t._v("Есть несколько причин, почему галлюцинации происходят[7][10][25]:")]),t._v(" "),v("p",[v("strong",[t._v("Первая причина — отсутствие актуальных данных.")]),t._v(" Если ИИ был обучен в 2022 году и не имеет доступа в интернет, он не знает, что произошло в 2023 году. Когда вы спрашиваете его о событиях 2023 года, он не может признать, что не знает, поэтому генерирует правдоподобное объяснение[7][10].")]),t._v(" "),v("p",[v("strong",[t._v("Вторая причина — противоречивые данные в тренировочном наборе.")]),t._v(" Если обучающие данные содержат противоречивую информацию об одном вопросе, ИИ может не иметь четкого сигнала о том, какой ответ правильный[10]. Вместо этого система создаёт компромисс — новое объяснение, которое звучит правдоподобно, но может быть совершенно выдуманным.")]),t._v(" "),v("p",[v("strong",[t._v("Третья причина — архитектура самой системы.")]),t._v(" ИИ обучен быть полезным. Если дать ему вопрос и система поймёт, что у неё нет хорошего ответа, она всё равно попытается быть полезной и выдаст что-нибудь, что звучит разумно[4][10][25].")]),t._v(" "),v("p",[v("strong",[t._v("Четвёртая причина — попытки быть логичными.")]),t._v(" Некоторые новые ИИ обучены пошагово рассуждать о проблеме. Если они должны пройти через 10 логических шагов для ответа, каждый шаг даёт шанс для ошибки[48]. Предыдущий неправильный шаг может привести к следующему неправильному шагу, и так далее, пока не появится совершенно выдуманный ответ[48].")]),t._v(" "),v("h2",{attrs:{id:"предвзятость-и-смещение-когда-ии-воспроизводит-человеческие-предубеждения"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#предвзятость-и-смещение-когда-ии-воспроизводит-человеческие-предубеждения"}},[t._v("#")]),t._v(" Предвзятость и смещение: когда ИИ воспроизводит человеческие предубеждения")]),t._v(" "),v("p",[t._v("Еще один класс ошибок, который может совершить ИИ, — это предвзятость. Это отличается от галлюцинаций. При галлюцинации ИИ выдумывает информацию. При предвзятости ИИ генерирует информацию, которая может быть технически верной или правдоподобной, но которая систематически дискриминирует или неправедно обращается с определёнными группами людей[2][32][34].")]),t._v(" "),v("p",[t._v("Вот как это происходит. Помните, что ИИ учится на примерах из реального мира. Реальный мир содержит предубеждения — исторические, культурные, социальные. Если ИИ обучается на текстах из интернета, он учится воспроизводить все эти предубеждения[2][32].")]),t._v(" "),v("p",[t._v("Классический пример: система рекомендации резюме. Компания обучила ИИ выбирать лучшие резюме для рассмотрения на должность инженера. Система обучалась на исторических данных — на примерах людей, которые были наняты на эту должность в прошлом. В прошлом большинство инженеров были мужчины. Поэтому ИИ научился связывать слова, которые появляются чаще в мужских резюме, с вероятностью того, что человек будет хорошим инженером. Когда система стала анализировать новые резюме, она систематически отдавала преимущество мужским кандидатам[2][32].")]),t._v(" "),v("p",[t._v("Это не происходит потому, что люди, создавшие систему, хотели дискриминации. Это происходит потому, что система автоматически усвоила предубеждение из исторических данных[2][32].")]),t._v(" "),v("p",[t._v("Есть много других примеров такого смещения[32][34]:")]),t._v(" "),v("p",[v("strong",[t._v("Системы распознавания лиц")]),t._v(" ошибаются чаще с женщинами и людьми с тёмным цветом кожи, потому что тренировочные данные содержали больше мужчин и людей со светлой кожей[2][32].")]),t._v(" "),v("p",[v("strong",[t._v("Языковые модели")]),t._v(" могут связывать определённые профессии с определёнными гендерами. Например, ассоциировать слово «медсестра» с женщиной, а слово «инженер» с мужчиной, потому что в тренировочных данных такие корреляции существуют[34].")]),t._v(" "),v("p",[v("strong",[t._v("Системы кредитного скоринга")]),t._v(" могут дискриминировать людей на основе их адреса проживания, потому что историческое неравенство привело к тому, что люди из определённых районов имели более низкие доходы[32][34].")]),t._v(" "),v("p",[t._v("Важно отметить, что смещение — это не всегда результат явной дискриминации. Часто это просто отражение того, как устроено общество. ИИ, обучаясь на данных о реальном обществе, усваивает эти закономерности и воспроизводит их[2][32].")]),t._v(" "),v("h2",{attrs:{id:"переобучение-и-недообучение-когда-модель-учится-неправильно"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#переобучение-и-недообучение-когда-модель-учится-неправильно"}},[t._v("#")]),t._v(" Переобучение и недообучение: когда модель учится неправильно")]),t._v(" "),v("p",[t._v("Есть ещё два типа ошибок, которые связаны с самим процессом обучения: переобучение и недообучение[3][13][16].")]),t._v(" "),v("p",[v("strong",[t._v("Переобучение происходит, когда ИИ слишком хорошо учится на тренировочных данных и плохо работает на новых данных.")]),t._v(" Представьте, что вы учитесь проходить конкретный тест, просто запоминая все вопросы и ответы. Вы получите идеальный результат на этом тесте. Но если вам даст новый тест с немного другими вопросами, вы провалитесь, потому что вы ничего не понимали — вы просто запомнили ответы[13][16].")]),t._v(" "),v("p",[t._v("То же самое может происходить с ИИ. Модель может быть обучена на конкретном наборе данных, где она работает идеально. Но когда её применяют в реальной ситуации с немного другими данными, она ошибается[1][3].")]),t._v(" "),v("p",[t._v("Например, система распознавания изображений обучена на фотографиях, сделанных в одном месте при хорошем освещении. Когда её применяют на фотографиях, сделанных при плохом освещении или с другого угла, она начинает ошибаться часто[1].")]),t._v(" "),v("p",[t._v("Почему это происходит? Потому что модель усвоила не только общие закономерности (например, как выглядит автомобиль в целом), но и специфические детали тренировочных данных (например, как выглядит автомобиль при конкретном освещении в конкретном месте)[13][16].")]),t._v(" "),v("p",[v("strong",[t._v("Недообучение происходит, когда ИИ не учится достаточно хорошо даже на тренировочных данных.")]),t._v(" Это может быть потому, что модель слишком простая для задачи, или потому что данных слишком мало, или потому что обучение было остановлено слишком рано[16].")]),t._v(" "),v("p",[t._v("Представьте, что вы пытаетесь научиться готовить, прочитав только заголовки рецептов, без пошаговых инструкций. Вы никогда не сможете хорошо готовить. То же самое может случиться с ИИ — если модель слишком простая или получает недостаточно данных для обучения, она не сможет хорошо выполнять задачу[16].")]),t._v(" "),v("p",[t._v("Баланс между переобучением и недообучением очень важен. Если модель слишком проста, она не справляется. Если модель слишком сложна, она переобучается и не может обобщаться на новые данные[13][16]. Это известно как проблема компромисса смещение-дисперсия[3][13].")]),t._v(" "),v("h2",{attrs:{id:"ограничения-контекстного-окна-когда-ии-забывает-начало-разговора"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#ограничения-контекстного-окна-когда-ии-забывает-начало-разговора"}},[t._v("#")]),t._v(" Ограничения контекстного окна: когда ИИ забывает начало разговора")]),t._v(" "),v("p",[t._v("Ещё один источник ошибок связан с тем, как ИИ обрабатывает информацию в течение разговора. ИИ не имеет настоящей долговременной памяти, как человек. Вместо этого, у каждого ИИ есть "),v("strong",[t._v("контекстное окно")]),t._v(" — определённый объём информации, которую система может хранить в одном диалоге[15][18].")]),t._v(" "),v("p",[t._v("Представьте, что вы разговариваете с человеком, но этот человек может помнить только последние 30 страниц текста из вашего разговора. Если разговор длиннее, он забывает, о чём вы говорили в начале. Это создаёт проблемы: он может противоречить себе, потому что забыл, что сказал ранее. Или он может дать ответ, который логичен исходя из последних страниц, но совершенно не соответствует контексту всего разговора[15][18].")]),t._v(" "),v("p",[t._v("Точно так же работает ИИ. Когда вы пишете длинный текст или ведёте длинный разговор, ИИ начинает терять старую информацию. Это может привести к ошибкам, противоречиям и потере связности[15][18].")]),t._v(" "),v("p",[t._v("Размер контекстного окна зависит от модели[15][18]. Бесплатная версия ChatGPT может хранить около 16 000 токенов. Это примерно 3000-4000 слов. Для сравнения, средняя страница текста содержит примерно 250-300 слов. Так что даже 50-60 страниц текста может быть недостаточно[15][18].")]),t._v(" "),v("p",[t._v("Это особенно проблема, когда вы пытаетесь дать ИИ длинный документ для анализа. Если вы даёте 100-страничный отчёт, ИИ может пропустить важную информацию из начала документа, потому что она вышла за границы его контекстного окна[15][6].")]),t._v(" "),v("h2",{attrs:{id:"слабая-способность-к-рассуждению-когда-ии-не-может-логически-думать"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#слабая-способность-к-рассуждению-когда-ии-не-может-логически-думать"}},[t._v("#")]),t._v(" Слабая способность к рассуждению: когда ИИ не может логически думать")]),t._v(" "),v("p",[t._v("Ещё один класс ошибок возникает из-за того, что ИИ не рассуждает как человек. Человеческий мозг может держать в уме сложные логические цепи и следить за связями между идеями. ИИ работает иначе[6][27].")]),t._v(" "),v("p",[t._v("Большие языковые модели на самом деле являются статистическими предсказателями. Они учатся предсказывать, какое слово вероятнее всего должно идти дальше, на основе всех предыдущих слов[43][46]. Это означает, что ИИ может хорошо работать с задачами, которые требуют распознавания закономерностей в тексте. Но ИИ может плохо работать с задачами, которые требуют настоящего логического мышления[6][27].")]),t._v(" "),v("p",[t._v("Например, если вы дадите ИИ простую логическую задачу: «У Петра есть три яблока. Он дал один яблок Маше. Сколько яблок осталось у Петра?», ИИ может правильно ответить. Но это потому, что эта задача встречается часто в тренировочных данных.")]),t._v(" "),v("p",[t._v("Если вы дадите более сложную логическую задачу, которая требует нескольких шагов рассуждения, ИИ может сбиться. Особенно если задача содержит нестандартный контекст или требует понимания того, почему определённый логический шаг необходим[6][27][36].")]),t._v(" "),v("p",[t._v("Это также объясняет, почему более новые ИИ, которые обучены пошагово рассуждать, иногда делают больше ошибок, чем просто давая прямой ответ[48]. Каждый шаг рассуждения — это шанс для ошибки. Если система ошибается на первом шаге, все последующие шаги будут неправильными[48].")]),t._v(" "),v("h2",{attrs:{id:"нечувствительность-к-новому-контексту-когда-ии-не-может-адаптироваться"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#нечувствительность-к-новому-контексту-когда-ии-не-может-адаптироваться"}},[t._v("#")]),t._v(" Нечувствительность к новому контексту: когда ИИ не может адаптироваться")]),t._v(" "),v("p",[t._v("ИИ обучается на данных из определённого периода времени. Это означает, что ИИ может плохо работать с совершенно новыми ситуациями, которых не было в тренировочных данных[1][4].")]),t._v(" "),v("p",[t._v("Например, если ИИ был обучен до пандемии COVID-19, он может не понимать многих вещей, связанных с пандемией. Если вы спросите его о масках, вакцинации или локдаунах в контексте пандемии, он может дать неправильный или устаревший ответ[1][4].")]),t._v(" "),v("p",[t._v("Аналогично, системы распознавания изображений, обученные на определённом типе изображений, могут плохо работать с изображениями, которые существенно отличаются от тренировочных данных. Автомобиль на фоне снега может быть неправильно распознан, если система была обучена только на автомобилях на чистом фоне[1][3].")]),t._v(" "),v("p",[t._v("Это называется проблемой "),v("strong",[t._v("distributional shift")]),t._v(" или сдвига распределения. Когда данные, на которых работает ИИ, существенно отличаются от данных, на которых ИИ был обучен, производительность падает[1][4].")]),t._v(" "),v("h2",{attrs:{id:"практические-примеры-как-ошибки-ии-влияют-на-реальных-людеи"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#практические-примеры-как-ошибки-ии-влияют-на-реальных-людеи"}},[t._v("#")]),t._v(" Практические примеры: как ошибки ИИ влияют на реальных людей")]),t._v(" "),v("p",[t._v("Чтобы сделать это более конкретным, давайте посмотрим на реальные примеры ошибок ИИ, которые имели серьёзные последствия[1][5][9][48]:")]),t._v(" "),v("p",[v("strong",[t._v("Адвокаты, которые использовали галлюцинированные прецеденты.")]),t._v(" В 2024 году несколько американских адвокатов использовали ChatGPT для поиска судебных прецедентов. ИИ выдумал несуществующие судебные дела и привёл их в официальные документы. Когда суд проверил эти прецеденты, они не существовали. Адвокаты потеряли дело, а один адвокат чуть не потерял лицензию[1][5][9].")]),t._v(" "),v("p",[v("strong",[t._v("Google Assistant, который рекомендовал опасный рецепт.")]),t._v(" Пользователь Reddit попросил ИИ от Google совет по приготовлению оливкового масла с чесноком. ИИ дал рецепт, который привёл к образованию культуры Clostridium botulinum — возбудителя ботулизма, опасного для жизни заболевания[1][5].")]),t._v(" "),v("p",[v("strong",[t._v("Автопилот Tesla, который не видел велосипедистов.")]),t._v(" Несколько раз сообщалось, что автопилот Tesla не видел велосипедистов со спины и не замечал пустые грузовые платформы на перекрёстках. Система неправильно интерпретировала красные круги на придорожных рекламных стендах как запрещающие сигналы светофора и включала экстренное торможение[1].")]),t._v(" "),v("p",[v("strong",[t._v("Системы кредитного скоринга, которые дискриминировали меньшинства.")]),t._v(" Несколько компаний обнаружили, что их ИИ-системы кредитного скоринга систематически давали более низкие баллы для чёрнокожих заёмщиков, даже если у них была лучшая кредитная история[1][2][32].")]),t._v(" "),v("p",[v("strong",[t._v("Курсор ИИ-помощник, который ложно сказал политику по установке.")]),t._v(" Платформа AI-ассистента по написанию кода Cursor имела ИИ-бот поддержки, который ложно сказал пользователям, что они могут установить программу только на один компьютер. Это вызвало возмущение, и некоторые клиенты отменили подписку[48].")]),t._v(" "),v("p",[t._v("Все эти примеры показывают, что ошибки ИИ — это не просто интеллектуальная проблема. Они могут иметь реальные последствия для людей.")]),t._v(" "),v("h2",{attrs:{id:"как-люди-могут-защитить-себя-от-ошибок-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#как-люди-могут-защитить-себя-от-ошибок-ии"}},[t._v("#")]),t._v(" Как люди могут защитить себя от ошибок ИИ")]),t._v(" "),v("p",[t._v("Раз уж ИИ может ошибаться, что вы можете сделать, чтобы защитить себя? Есть несколько практических советов[4][5][7][9][25]:")]),t._v(" "),v("p",[v("strong",[t._v("Проверяйте критическую информацию.")]),t._v(" Если ИИ дал вам информацию, которая важна для ваших решений, проверьте её в надежных источниках. Особенно, если это касается здоровья, финансов, права или безопасности[5][9].")]),t._v(" "),v("p",[v("strong",[t._v("Используйте несколько ИИ-систем.")]),t._v(" Разные ИИ обучены по-разному и могут давать разные ответы. Если вы спросите одно и то же у нескольких систем и получите одинаковые ответы, это повышает вероятность, что ответ правильный[4][7].")]),t._v(" "),v("p",[v("strong",[t._v("Будьте конкретны в своих запросах.")]),t._v(" Чем более подробным и конкретным вы будете в своих запросах к ИИ, тем выше вероятность, что вы получите правильный ответ. Абстрактные или нечеткие вопросы часто приводят к ошибкам[5][6].")]),t._v(" "),v("p",[v("strong",[t._v("Не верьте слепо, когда ИИ звучит уверенно.")]),t._v(" ИИ может быть очень убедительным даже когда говорит неправду. Если ИИ приводит конкретные факты или цифры, это не означает, что они правильные[4][7][25].")]),t._v(" "),v("p",[v("strong",[t._v("Используйте ИИ как ассистента, а не как экспорта.")]),t._v(" ИИ хорош как помощник, который может вам помочь с идеями, генерацией текста, анализом информации. Но не полагайтесь полностью на ИИ для принятия критических решений без проверки его ответов[9].")]),t._v(" "),v("p",[v("strong",[t._v("Ищите источники, когда ИИ дает факты.")]),t._v(" Хороший ИИ должен предоставить ссылки на источники, если он говорит о фактах. Если ИИ не может предоставить источник, это красный флаг[4][7].")]),t._v(" "),v("p",[v("strong",[t._v("Проверьте согласованность.")]),t._v(" Если ИИ противоречит себе в одном разговоре, это признак, что он может путаться или опираться на противоречивые данные[4].")]),t._v(" "),v("h2",{attrs:{id:"как-разработчики-борются-с-ошибками-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#как-разработчики-борются-с-ошибками-ии"}},[t._v("#")]),t._v(" Как разработчики борются с ошибками ИИ")]),t._v(" "),v("p",[t._v("Разработчики и исследователи работают над тем, чтобы сделать ИИ менее подверженным ошибкам. Есть несколько подходов[7][10][25]:")]),t._v(" "),v("p",[v("strong",[t._v("Улучшение качества тренировочных данных.")]),t._v(" Вместо использования всех доступных данных, компании начали фокусироваться на качестве данных. Удаление ошибок и противоречий из тренировочных данных помогает созданию более надежной модели[10][17].")]),t._v(" "),v("p",[v("strong",[t._v("RAG (Retrieval-Augmented Generation).")]),t._v(" Это подход, когда ИИ сначала ищет релевантную информацию в достоверных источниках (например, в документах компании или в интернете), а затем генерирует ответ на основе этой информации. Это помогает снизить галлюцинации[7][25].")]),t._v(" "),v("p",[v("strong",[t._v("Обучение с подкреплением от человека (RLHF).")]),t._v(" Люди оценивают ответы ИИ и дают ему обратную связь. ИИ учится генерировать ответы, которые люди считают лучшими. Это помогает улучшить качество ответов[7][43].")]),t._v(" "),v("p",[v("strong",[t._v("Специализированные детекторы галлюцинаций.")]),t._v(" Компании разрабатывают специальные системы, которые пытаются обнаружить, когда ИИ галлюцинирует. Эти системы сравнивают ответы ИИ с проверенной информацией[10].")]),t._v(" "),v("p",[v("strong",[t._v("Улучшение архитектуры моделей.")]),t._v(" Исследователи экспериментируют с новыми способами структурирования нейронных сетей, которые помогают им лучше рассуждать и избегать ошибок[17][46].")]),t._v(" "),v("p",[v("strong",[t._v("Честный дизайн интерфейса.")]),t._v(" Компании начали добавлять предупреждения в интерфейсы ИИ, чтобы люди знали о риске галлюцинаций и других ошибок[4][7][25].")]),t._v(" "),v("p",[t._v("Однако важно отметить, что полностью избавиться от ошибок ИИ может быть невозможно. Некоторые исследователи считают, что галлюцинации являются неизбежной частью того, как работают большие языковые модели, и лучшее, что мы можем сделать, — это минимизировать их влияние[7][10].")]),t._v(" "),v("h2",{attrs:{id:"гипотеза-о-неизбежности-ошибок"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#гипотеза-о-неизбежности-ошибок"}},[t._v("#")]),t._v(" Гипотеза о неизбежности ошибок")]),t._v(" "),v("p",[t._v("Есть интересная гипотеза, выдвинутая некоторыми исследователями: галлюцинации и другие ошибки могут быть неизбежны для любой системы машинного обучения, которая пытается быть полезной[7][10][25].")]),t._v(" "),v("p",[t._v("Почему? Потому что ИИ оптимизирован для полезности, а не для абсолютной точности. Если ИИ не уверен в ответе, он должен выбрать: либо ничего не ответить (не полезно), либо дать ответ, в котором он не совсем уверен (полезно, но может быть неправильно)[7][10].")]),t._v(" "),v("p",[t._v("Большинство разработчиков выбирают второй вариант. Это означает, что ИИ будет иногда ошибаться, но он будет полезнее[7][10].")]),t._v(" "),v("p",[t._v("Кроме того, теоретически есть закономерность: более мощные ИИ иногда ошибаются чаще, чем менее мощные, в определённых задачах[48]. Это может быть потому, что более мощные ИИ пытаются быть более вспомогательными, даже когда они не совсем уверены в ответе[48].")]),t._v(" "),v("h2",{attrs:{id:"заключение-ошибки-ии-это-не-баг-а-фича"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#заключение-ошибки-ии-это-не-баг-а-фича"}},[t._v("#")]),t._v(" Заключение: ошибки ИИ — это не баг, а фича")]),t._v(" "),v("p",[t._v("Когда вы обнаруживаете, что ИИ ошибается, это может быть разочаровывающим. Вы думали, что система очень умная, и вдруг она говорит что-то явно неправильное. Но важно понимать, что ошибки ИИ — это не признак того, что система сломана. Это просто показатель того, что ИИ работает иначе, чем человеческий мозг.")]),t._v(" "),v("p",[t._v("ИИ работает на вероятностях и закономерностях в данных. Если данные неполные, ошибочные, предвзятые или противоречивые, ИИ усвоит эти недостатки[1][2][4][5]. ИИ не понимает мир так, как люди — он работает с математической абстракцией. Это означает, что ИИ может совершать ошибки в логике и рассуждении, даже если его ответ звучит убедительно[4][6][27].")]),t._v(" "),v("p",[t._v("На уровне 0, главное, что нужно понимать — это "),v("strong",[t._v("не верить слепо ИИ")]),t._v(", особенно когда речь идет о критических вопросах. Используйте ИИ как инструмент, а не как авторитет. Проверяйте его ответы. Используйте несколько источников. И помните, что даже если ИИ звучит уверенно, это не гарантирует, что он прав[5][9][25].")]),t._v(" "),v("p",[t._v("Разработчики ИИ постоянно работают над тем, чтобы сделать системы более надежными и менее подверженными ошибкам. Но полностью избавиться от ошибок, вероятно, невозможно. Вместо этого цель — создать ИИ, который в основном правильный, признает границы своих знаний, и ясно коммуницирует, когда он не уверен[7][10][25].")]),t._v(" "),v("p",[t._v("На уровне 0, это всё, что вам нужно знать. ИИ ошибается, потому что он учится на примерах, и примеры неполные. ИИ ошибается, потому что он оптимизирован для полезности, а не для абсолютной точности. ИИ ошибается, потому что он работает иначе, чем человеческий мозг. Это просто реальность того, как работает машинное обучение. И это ОК.")]),t._v(" "),v("h2",{attrs:{id:"источники-инфы"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#источники-инфы"}},[t._v("#")]),t._v(" Источники инфы")]),t._v(" "),v("p",[t._v("[1] https://habr.com/ru/companies/kryptonite/articles/913640/\n[2] https://rb.ru/stories/ai-bias/\n[3] https://vc.ru/id22269/682524-nedostatki-neironnyh-setei-kakie-ogranicheniya-i-problemy-est-u-etoi-tehnologii-i-kak-ih-mozhno-reshit\n[4] https://practicum.yandex.ru/blog/kak-oshibayutsya-nejroseti/\n[5] https://www.unisender.com/ru/blog/oshibki-pri-rabote-s-nejrosetyami/\n[6] https://www.unisender.com/ru/blog/nedostatki-neirosetei-chto-ne-mozhet-delat-ii/\n[7] https://t-j.ru/ai-hallucinations/\n[9] https://media.future-hub.io/stati/oshibki-iskusstvennogo-intellekta\n[10] https://habr.com/ru/companies/mws/articles/855968/\n[13] https://neerc.ifmo.ru/wiki/index.php?title=%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5\n[14] https://www.forbes.ru/tekhnologii/515096-kogda-koncitsa-vse-na-skol-ko-ese-hvatit-dannyh-dla-obucenia-ii\n[15] https://habr.com/ru/companies/bothub/articles/952254/\n[16] https://ru.wikipedia.org/wiki/%D0%9F%D0%B5%D1%80%D0%B5%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5\n[17] https://habr.com/ru/companies/cloud_ru/articles/875588/\n[18] https://www.binance.com/ru/square/post/28906263084714\n[19] https://practicum.yandex.ru/blog/kak-oshibayutsya-nejroseti/\n[25] https://vc.ru/ai/2270082-kak-izbezhat-gallyutsinatsiy-ii\n[27] https://vc.ru/ai/2294344-granitsy-ii-i-vychislimosti\n[28] https://www.ultralytics.com/ru/blog/fuzzy-logic-in-ai-enhances-the-analysis-of-noisy-visual-data\n[32] https://vc.ru/id4616024/2268802-smishenie-v-iskusstvennom-intellekte\n[34] https://systems-analysis.ru/wiki/%D0%9F%D1%80%D0%B5%D0%B4%D0%B2%D0%B7%D1%8F%D1%82%D0%BE%D1%81%D1%82%D1%8C_%D0%B2_%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8\n[43] https://bigdataschool.ru/wiki/large-language-models/\n[45] https://habr.com/ru/companies/getmatch/articles/700736/\n[46] https://habr.com/ru/companies/raft/articles/898854/\n[48] https://www.itweek.ru/ai/article/detail.php?ID=232184")])])}),[],!1,null,null,null);v.default=a.exports}}]);
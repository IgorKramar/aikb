(window.webpackJsonp=window.webpackJsonp||[]).push([[49],{371:function(t,v,i){"use strict";i.r(v);var a=i(18),_=Object(a.a)({},(function(){var t=this,v=t._self._c;return v("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[v("h1",{attrs:{id:"можно-ли-доверять-ии-руководство-по-правильному-отношению-к-искусственному-интеллекту"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#можно-ли-доверять-ии-руководство-по-правильному-отношению-к-искусственному-интеллекту"}},[t._v("#")]),t._v(" Можно ли доверять ИИ? Руководство по правильному отношению к искусственному интеллекту")]),t._v(" "),v("p",[v("strong",[t._v("Краткая суть:")]),t._v(" Полностью доверять ИИ нельзя, но и полностью его отвергать не стоит. На уровне 0 вам нужно научиться "),v("strong",[t._v("калибровать")]),t._v(" свое доверие к ИИ — то есть доверять ему ровно столько, сколько он этого заслуживает. ИИ хорошо работает с задачами на обработку данных, но может галлюцинировать, содержать предвзятость и не понимать контекст. В 2025 году только 46% людей в мире готовы доверять ИИ системам, и это не случайно. На этом уровне вы узнаете, почему нельзя слепо верить ИИ, как устроено доверие к ИИ, и как правильно его использовать, не подвергая себя риску.")]),t._v(" "),v("h2",{attrs:{id:"что-вообще-означает-доверие-к-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#что-вообще-означает-доверие-к-ии"}},[t._v("#")]),t._v(' Что вообще означает "доверие" к ИИ?')]),t._v(" "),v("p",[t._v("Когда мы говорим о доверии к человеку, мы имеем в виду уверенность в его честности, компетентности и благих намерениях. Мы доверяем врачу, потому что верим, что он хочет нам помочь, что он прошел обучение и что он честно скажет нам правду, даже если она плохая[55].")]),t._v(" "),v("p",[t._v("Доверие к ИИ — это совершенно другое понятие[55]. ИИ не имеет намерений. Он не может быть честным или нечестным в нравственном смысле. ИИ — это математическая модель, набор вероятностей и закономерностей, обученный на данных. Вопрос не в том, честен ли ИИ, а в том, "),v("strong",[t._v("надежен ли он и работает ли так, как мы от него ожидаем")]),t._v("[4][55].")]),t._v(" "),v("p",[t._v("Доверие к ИИ зависит от его способности выполнять свою функцию предсказуемо и без ошибок. Оно зависит от качества данных, на которых его обучили, от используемых алгоритмов и от того, насколько прозрачен процесс его работы. Вы доверяете калькулятору, потому что знаете, что 2+2 всегда будет 4. Но вы не можете доверять ИИ с такой же уверенностью, потому что ИИ может ошибаться не по техническим причинам, а по причинам, заложенным в самой его природе[4][55].")]),t._v(" "),v("p",[t._v("Исследование Университета Мельбурна и KPMG в 2025 году показало, что только 46% людей в мире готовы доверять ИИ системам[5]. Это говорит о том, что люди интуитивно понимают: доверие к ИИ — это не простое «да» или «нет», а сложный и многоуровневый процесс[5].")]),t._v(" "),v("h2",{attrs:{id:"почему-доверие-к-ии-сложнее-чем-кажется"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#почему-доверие-к-ии-сложнее-чем-кажется"}},[t._v("#")]),t._v(" Почему доверие к ИИ сложнее, чем кажется")]),t._v(" "),v("p",[t._v("На первый взгляд может показаться, что ИИ — это просто программа, которая работает по заданным правилам, и его должно быть просто понять и оценить. Но это не так[4]. Современный ИИ — это не традиционная компьютерная программа с четкими инструкциями. Это вероятностная система, которая учится на данных и может вести себя непредсказуемо[4].")]),t._v(" "),v("p",[t._v("Вот почему доверие к ИИ так сложно[4][55]:")]),t._v(" "),v("p",[v("strong",[t._v("ИИ не работает по явным правилам.")]),t._v(" Старая программа: если А, то делай Б. Если В, то делай Г. Все ясно и понятно. ИИ работает иначе. Он обучается на примерах и выдает вероятностные ответы. Никто, даже разработчики, не может точно сказать, почему система пришла к конкретному решению[4][8].")]),t._v(" "),v("p",[v("strong",[t._v("ИИ может эволюционировать и меняться.")]),t._v(" После развертывания ИИ может продолжать обновляться, получать новые данные, адаптироваться. Это означает, что его поведение может измениться со временем, даже если вы думали, что все контролируете[4].")]),t._v(" "),v("p",[v("strong",[t._v("Доверие к ИИ зависит от множества факторов.")]),t._v(" Это не просто вопрос компетентности. Это вопрос качества данных, используемых алгоритмов, контекста, в котором работает ИИ, того, как люди его используют, и того, как они к нему относятся[26][29].")]),t._v(" "),v("p",[v("strong",[t._v("Психология играет огромную роль.")]),t._v(" Люди часто либо чрезмерно доверяют ИИ (потому что он звучит научно и авторитетно), либо чрезмерно ему не доверяют (потому что боятся неизвестного). Оба подхода неправильны[26][29].")]),t._v(" "),v("p",[t._v("Психолог из Кембриджа объясняет: когда люди действительно доверяют ИИ-модели типа цифровых близнецов, модель может начать формировать решения самостоятельно — иногда даже больше, чем люди, использующие ее. Но это все еще зависит от людей, которые должны решить, когда следовать модели, а когда отступить и самим подумать[45].")]),t._v(" "),v("h2",{attrs:{id:"шесть-основных-причин-почему-нельзя-слепо-доверять-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#шесть-основных-причин-почему-нельзя-слепо-доверять-ии"}},[t._v("#")]),t._v(" Шесть основных причин, почему нельзя слепо доверять ИИ")]),t._v(" "),v("p",[t._v("Компания Lifehacker выделила шесть ключевых причин, почему полностью доверять ИИ опасно[1]. Давайте рассмотрим каждую из них подробнее, потому что это критически важно для понимания.")]),t._v(" "),v("h3",{attrs:{id:"причина-первая-ии-запрограммирован-создавать-правдоподобные-ответы-а-не-правдивые"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-первая-ии-запрограммирован-создавать-правдоподобные-ответы-а-не-правдивые"}},[t._v("#")]),t._v(" Причина первая: ИИ запрограммирован создавать правдоподобные ответы, а не правдивые")]),t._v(" "),v("p",[t._v("Это, пожалуй, самая важная причина из всех[1]. ИИ хорошо знает, что ему нужно ответить на любой вопрос. Он никогда не скажет «я не знаю» просто так. Вместо этого, если ИИ не знает ответ, он сгенерирует что-то, что "),v("strong",[t._v("звучит")]),t._v(" правдоподобно, но может быть совершенно неправдивым[1].")]),t._v(" "),v("p",[t._v("Представьте, что вы спросили ChatGPT про редкого режиссера, которого может вообще не существовать. ИИ не только даст вам ответ, но и приведет детали: названия фильмов, годы выпуска, актеров. Все это будет звучать убедительно. Но если вы проверите информацию, окажется, что режиссер никогда не существовал, фильмы никогда не снимались[1].")]),t._v(" "),v("p",[t._v("Это происходит потому, что ИИ — это по сути продвинутый автозаполнитель текста. Ему задана задача: «предскажи следующие слова, которые с наибольшей вероятностью идут дальше». И ИИ очень хорошо это делает. Но абсолютно никто не проверяет, правдивы ли эти слова[1][10].")]),t._v(" "),v("h3",{attrs:{id:"причина-вторая-ии-может-спорить-с-вами-даже-когда-вы-правы"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-вторая-ии-может-спорить-с-вами-даже-когда-вы-правы"}},[t._v("#")]),t._v(" Причина вторая: ИИ может спорить с вами, даже когда вы правы")]),t._v(" "),v("p",[t._v("Еще одна проблема: ИИ не просто может выдумывать информацию. Он может "),v("strong",[t._v("упорно настаивать")]),t._v(" на ложной информации[1]. Если система была обучена на неправильных данных, она может верить в эти неправильные данные с полной уверенностью. Если вы уличите ИИ во лжи, он может согласиться с вами в конкретный момент, но потом снова начать говорить то же самое, потому что его базовые убеждения (заложенные в данных, на которых его обучили) не изменились[1].")]),t._v(" "),v("p",[t._v("Это создает ситуацию, когда ИИ может быть уверен в своей неправоте больше, чем в правоте, если его так обучили.")]),t._v(" "),v("h3",{attrs:{id:"причина-третья-ии-может-ограничивать-вашу-креативность"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-третья-ии-может-ограничивать-вашу-креативность"}},[t._v("#")]),t._v(" Причина третья: ИИ может ограничивать вашу креативность")]),t._v(" "),v("p",[t._v("Если вы привыкнете полностью полагаться на ИИ для создания контента, вашей собственной креативности может не хватить[1]. Это похоже на использование GPS для навигации: если вы постоянно полагаетесь на GPS, ваше чувство направления ослабнет. Человеческий мозг адаптируется к инструментам, которые мы используем[1].")]),t._v(" "),v("p",[t._v("Кроме того, ИИ может создавать только то, что он видел в тренировочных данных. Он не может быть действительно оригинальным, потому что у него нет собственного опыта, собственных идей, своего взгляда на мир. Если вы будете копировать и вставлять результаты ИИ без размышлений, вы будете просто повторять то, что уже существует[1].")]),t._v(" "),v("h3",{attrs:{id:"причина-четвертая-ии-выдумывает-источники"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-четвертая-ии-выдумывает-источники"}},[t._v("#")]),t._v(" Причина четвёртая: ИИ выдумывает источники")]),t._v(" "),v("p",[t._v("Если вы попросите ИИ назвать источники своей информации, он может назвать исследования, которые не существуют[1][7]. Статьи, которые ИИ якобы нашел, могут быть полностью вымышленными. Авторы, которых он цитирует, могут быть людьми, которых никогда не было. Ссылки, которые он дает, могут вести в никуда[1][7].")]),t._v(" "),v("p",[t._v("Исследование показало, что из 178 ссылок, приведенных GPT-3, 69 содержали неправильный или не существующий идентификатор DOI (Digital Object Identifier). Еще 28 вообще не имели известного DOI и не могли быть найдены в Google[7].")]),t._v(" "),v("p",[t._v("В другом исследовании анализировались 115 ссылок, предоставленных ChatGPT-3.5. Оказалось, что 47% из них были полностью сфабрикованы. Еще 46% цитировали реальные источники, но извлекали из них "),v("strong",[t._v("неправильную информацию")]),t._v(". Только 7% ссылок были приведены правильно с точной информацией[7].")]),t._v(" "),v("h3",{attrs:{id:"причина-пятая-ии-может-быть-использован-для-манипуляции-и-распространения-лжи"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-пятая-ии-может-быть-использован-для-манипуляции-и-распространения-лжи"}},[t._v("#")]),t._v(" Причина пятая: ИИ может быть использован для манипуляции и распространения лжи")]),t._v(" "),v("p",[t._v("Те же инструменты, которые создали полезный ChatGPT, можно использовать для создания дезинформации, лживых видео (дипфейков), поддельных новостей и троллинга[1][21]. ИИ становится инструментом манипуляции.")]),t._v(" "),v("p",[t._v("Злоумышленники уже используют ИИ для клонирования голосов политиков, создания поддельных изображений известных людей, генерирования убедительных фишинг-писем. Это не теория, это уже происходит[21].")]),t._v(" "),v("h3",{attrs:{id:"причина-шестая-ии-не-может-заменить-человеческое-суждение"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#причина-шестая-ии-не-может-заменить-человеческое-суждение"}},[t._v("#")]),t._v(" Причина шестая: ИИ не может заменить человеческое суждение")]),t._v(" "),v("p",[t._v("Самая глубокая причина, по которой нельзя полностью доверять ИИ: ИИ не может понимать эмоции, контекст и нематериальные аспекты, которые необходимы для принятия по-настоящему важных решений[1][45].")]),t._v(" "),v("p",[t._v("Если вы попросите ИИ выбрать между двумя книгами, он порекомендует ту, у которой выше рейтинг. Но он не сможет учесть вашу личный вкус, вашу текущее настроение, цель, для которой вам нужна эта книга. Человеческий рецензент смог бы это учесть[1][45].")]),t._v(" "),v("p",[t._v("Точно так же ИИ не может принимать решения, которые требуют этического суждения, понимания человеческого достоинства или оценки социальных последствий. Это требует человеческой мудрости, которую невозможно кодировать[1].")]),t._v(" "),v("h2",{attrs:{id:"глобальные-тенденции-доверия-в-2025-году"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#глобальные-тенденции-доверия-в-2025-году"}},[t._v("#")]),t._v(" Глобальные тенденции доверия в 2025 году")]),t._v(" "),v("p",[t._v("Чтобы понять, насколько серьезно люди относятся к вопросу доверия к ИИ, давайте посмотрим на данные из 2025 года[2][5][26].")]),t._v(" "),v("p",[v("strong",[t._v("Глобальное снижение доверия.")]),t._v(" Исследование Университета Мельбурна и KPMG, проведенное в январе 2025 года, опросило 48 340 человек из 47 стран[5]. Результаты показывают четкую картину: только 46% людей глобально готовы доверять ИИ системам. Это не большинство, это именно половина, и то не совсем[5].")]),t._v(" "),v("p",[t._v("Интересно, что доверие "),v("strong",[t._v("различается по регионам")]),t._v("[5]. В развивающихся экономиках люди более доверчивы. Например, в Китае 83% доверяют ИИ, в Индонезии 80%. Но в развитых странах скептицизм выше: в США только 39%, в Канаде 40%[5]. Почему? Потому что люди в развитых странах больше знают о рисках ИИ, больше слышали о случаях неправильного использования[5].")]),t._v(" "),v("p",[v("strong",[t._v("Огромное расхождение между экспертами и публикой.")]),t._v(" Исследование Pew Research Center в 2024 году показало почти противоположные мнения[2]. Только 11% населения США более взволнованы, чем обеспокоены ИИ. А среди экспертов этот процент 47%. Это говорит о том, что люди на улице боятся ИИ больше, чем те, кто его создает[2].")]),t._v(" "),v("p",[t._v("43% населения США боятся, что ИИ нанесет им личный вред[2]. При этом только 15% экспертов так думают[2]. Это существенное различие.")]),t._v(" "),v("p",[v("strong",[t._v("Высокий скептицизм в образовании.")]),t._v(" В высших учебных заведениях 53% студентов обеспокоены точностью ИИ[2]. 55% видят риски для академической честности[2]. Преподаватели еще более скептичны: 88% используют ИИ минимально из-за проблем надежности[2].")]),t._v(" "),v("p",[v("strong",[t._v("Предвзятость, приватность и безопасность.")]),t._v(" Самые большие опасения касаются четырех вещей[2][5]: дезинформации (люди боятся, что ИИ будет распространять ложь), потери работы (страх замены человека ИИ), нарушения приватности (боязнь, что ИИ будет собирать и неправильно использовать личные данные) и предвзятости (опасение, что ИИ будет дискриминировать определенные группы)[2][5].")]),t._v(" "),v("p",[v("strong",[t._v("Образование как ключ.")]),t._v(" Исследование показало интересную закономерность: низкая грамотность в ИИ связана с недоверием[5]. Люди, которые лучше понимают, как ИИ работает и каковы его ограничения, более склонны к "),v("strong",[t._v("калибровочному доверию")]),t._v(" — к правильному уровню доверия, соответствующему реальным возможностям ИИ[5].")]),t._v(" "),v("h2",{attrs:{id:"конкретные-примеры-того-почему-доверие-к-ии-привело-к-проблемам"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#конкретные-примеры-того-почему-доверие-к-ии-привело-к-проблемам"}},[t._v("#")]),t._v(" Конкретные примеры того, почему доверие к ИИ привело к проблемам")]),t._v(" "),v("p",[t._v("Теория — это хорошо, но реальные примеры — это еще лучше. Вот несколько конкретных случаев, когда люди слишком доверили ИИ и поплатились за это.")]),t._v(" "),v("p",[v("strong",[t._v("Адвокаты, которые использовали выдуманные судебные прецеденты.")]),t._v(" В 2024 году группа американских адвокатов использовала ChatGPT для поиска судебных прецедентов по делу. ChatGPT дал им подробный список дел с номерами, названиями и даже цитатами. Адвокаты скопировали эту информацию прямо в официальный документ суда. Проблема в том, что все эти дела были полностью выдуманы ИИ. Они не существовали. Суд обнаружил обман. Дело было проиграно, и один из адвокатов едва не потерял лицензию[1][9].")]),t._v(" "),v("p",[v("strong",[t._v("Google Assistant, который рекомендовал потенциально смертельный рецепт.")]),t._v(" Пользователь спросил у Google Assistant, как приготовить оливковое масло с чесноком. ИИ дал рецепт, который выглядел безобидным, но на практике привел бы к размножению бактерий Clostridium botulinum — возбудителя ботулизма, опасного для жизни заболевания[1][4].")]),t._v(" "),v("p",[v("strong",[t._v("Системы кредитного скоринга, которые дискриминировали заемщиков.")]),t._v(" Несколько финансовых учреждений использовали ИИ для автоматического определения кредитоспособности. Системы были обучены на исторических данных, которые содержали предвзятость. В результате чернокожие заемщики систематически получали более низкие баллы, даже если у них была лучшая кредитная история, чем белые заемщики[1][2][3][6].")]),t._v(" "),v("p",[v("strong",[t._v("Автомобили Tesla, которые не видели велосипедистов.")]),t._v(" Было несколько инцидентов, когда автопилот Tesla не видел людей на велосипедах позади автомобиля и не включал тормоза. Система считала, что столкновения не будет, потому что велосипедист находился вне ее поля видения, определенного во время обучения[1].")]),t._v(" "),v("p",[v("strong",[t._v("Системы распознавания лиц, которые ошибались чаще для женщин и людей с темной кожей.")]),t._v(" Исследование Gender Shades показало, что системы распознавания лиц работали намного хуже для черных женщин, чем для белых мужчин, потому что обучающие данные содержали перекос[10][32].")]),t._v(" "),v("p",[t._v("Все эти примеры показывают одно: ИИ можно использовать неправильно, и опасность реальна.")]),t._v(" "),v("h2",{attrs:{id:"как-правильно-калибровать-доверие-к-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#как-правильно-калибровать-доверие-к-ии"}},[t._v("#")]),t._v(" Как правильно калибровать доверие к ИИ")]),t._v(" "),v("p",[t._v("Теперь, когда мы знаем, почему нельзя полностью доверять ИИ, следующий вопрос: как правильно его использовать? Ответ — "),v("strong",[t._v("калибровать")]),t._v(" свое доверие[49][52].")]),t._v(" "),v("p",[t._v("Калибровка доверия означает две вещи[49][52]:")]),t._v(" "),v("p",[v("strong",[t._v("Во-первых, это калибровка (calibration в узком смысле)")]),t._v(" — соответствие между вашим доверием и реальными возможностями системы. Если ИИ имеет высокую точность в определенной задаче, вы должны доверять ему в этой задаче. Если ИИ не был протестирован на данных, похожих на ваши, вы должны ему не доверять[49][52].")]),t._v(" "),v("p",[v("strong",[t._v("Во-вторых, это разрешение (resolution)")]),t._v(" — способность адаптировать ваше доверие в зависимости от изменения условий[49][52]. Это означает две вещи:")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("Функциональная специфика")]),t._v(": вы адаптируете свое доверие в зависимости от конкретной задачи. ИИ может быть хорошим в одной задаче и плохим в другой. Например, ИИ отлично распознает кошек на фотографиях, но может плохо распознавать их во время видеотрансляная специфика**: вы адаптируете доверие со временем. Если ИИ становится более точным по мере того, как он получает больше данных, ваше доверие должно расти. Если ИИ становится менее актуальным, потому что появились новые данные, которыми он не обучен, ваше доверие должно падать[49][52].")])]),t._v(" "),v("p",[t._v("Как практически это делать? Есть несколько ключевых шагов[1][4][25][32]:")]),t._v(" "),v("p",[v("strong",[t._v("Во-первых, проверяйте факты.")]),t._v(" Если ИИ говорит вам что-то важное, проверьте это в других источниках[1][25][32]. Особенно если это касается здоровья, финансов, права или безопасности. Не стесняйтесь тратить время на проверку[25][32].")]),t._v(" "),v("p",[v("strong",[t._v("Во-вторых, ищите источники.")]),t._v(" Если ИИ цитирует исследование, ищите это исследование. Если ИИ ссылается на статью, попробуйте найти эту статью[25][32]. Если он не может дать вам ссылку или ссылка ведет в никуда, это красный флаг[25][32].")]),t._v(" "),v("p",[v("strong",[t._v("В-третьих, смотрите на согласованность.")]),t._v(" Если ИИ противоречит себе даже в одном разговоре, это признак того, что система путается или опирается на противоречивые данные[4].")]),t._v(" "),v("p",[v("strong",[t._v("В-четвертых, используйте несколько источников.")]),t._v(" Спросите одно и то же у нескольких ИИ систем. Если все они дают похожий ответ, вероятность того, что ответ верный, выше[4][7].")]),t._v(" "),v("p",[v("strong",[t._v("В-пятых, понимайте контекст и ограничения.")]),t._v(" Узнайте, когда был обучен ИИ, на каких данных, для каких задач он предназначен[4]. Понимание границ поможет вам правильно его использовать[4].")]),t._v(" "),v("h2",{attrs:{id:"что-такое-прозрачныи-и-ответственныи-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#что-такое-прозрачныи-и-ответственныи-ии"}},[t._v("#")]),t._v(" Что такое прозрачный и ответственный ИИ")]),t._v(" "),v("p",[t._v("Компании и правительства понимают проблему с доверием к ИИ. Поэтому начали разрабатывать концепции «прозрачного ИИ» и «ответственного ИИ»[9][11][31][34].")]),t._v(" "),v("p",[v("strong",[t._v("Прозрачность (Transparency)")]),t._v(" означает, что вы знаете, как ИИ работает[11]. Это означает знание о том, какие данные использовались, какие алгоритмы использовались, кто разработал систему[11]. Компания должна рассказать вам, как она собирает данные, как их обрабатывает, кому их продает[11].")]),t._v(" "),v("p",[v("strong",[t._v("Объяснимость (Explainability)")]),t._v(" означает, что система может объяснить, почему она пришла к конкретному решению[8][11]. Если ИИ отказал вам в кредите, вы должны иметь право узнать, почему. Не просто получить список факторов, а получить реальное объяснение[8][11].")]),t._v(" "),v("p",[t._v("Однако здесь есть проблема, называемая "),v("strong",[t._v("иллюзией объяснимости")]),t._v("[8]. Компании могут генерировать объяснения, которые звучат убедительно, но на самом деле не отражают то, как работает ИИ. Это похоже на то, как если бы адвокат дал вам впечатляющее объяснение победы, но оно не совпадало с тем, как суд на самом деле принял решение[8].")]),t._v(" "),v("p",[v("strong",[t._v("Ответственный ИИ (Responsible AI)")]),t._v(" — это более широкая концепция[31][34][51]. Это означает, что компания разработала ИИ, принимая во внимание этику, справедливость, приватность и безопасность[31][34]. Это означает, что ИИ прошел проверку на предвзятость. Это означает, что люди несут за него ответственность, а не компьютер[31][34].")]),t._v(" "),v("p",[t._v("Ответственный ИИ также включает "),v("strong",[t._v("человеческий надзор")]),t._v("[15]. В критических ситуациях (медицина, право, финансы) человек, а не компьютер, должен принимать окончательное решение. ИИ должен быть вспомогательным инструментом[15][34].")]),t._v(" "),v("p",[t._v("В Европейском Союзе принят "),v("strong",[t._v("AI Act")]),t._v(" — первый в мире комплексный закон об ИИ[14][17]. Он классифицирует ИИ по уровню риска и требует разного уровня надзора. Например, ИИ для лицевого распознавания считается высокорисковым и подлежит строгому контролю[14][17].")]),t._v(" "),v("h2",{attrs:{id:"как-построить-правильное-отношение-к-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#как-построить-правильное-отношение-к-ии"}},[t._v("#")]),t._v(" Как построить правильное отношение к ИИ")]),t._v(" "),v("p",[t._v("Для новичка (уровень 0) главное — понять, как правильно относиться к ИИ[50][53][54]. Это не просто набор советов, а целый подход, называемый "),v("strong",[t._v("AI Literacy")]),t._v(" (грамотность в области ИИ)[50][53][54].")]),t._v(" "),v("p",[t._v("AI Literacy означает[50][53]:")]),t._v(" "),v("ul",[v("li",[v("strong",[t._v("Понимание")]),t._v(": знать, что такое ИИ, как он работает, что он может и не может делать")]),t._v(" "),v("li",[v("strong",[t._v("Оценивание")]),t._v(": уметь критически оценивать ИИ-системы, их возможности, ограничения и риски")]),t._v(" "),v("li",[v("strong",[t._v("Использование")]),t._v(": уметь безопасно и эффективно использовать ИИ инструменты")])]),t._v(" "),v("p",[t._v("Несколько культурных элементов поддерживают правильное отношение к ИИ[54]:")]),t._v(" "),v("p",[v("strong",[t._v("Культура учения.")]),t._v(" Люди должны постоянно учиться и адаптироваться. ИИ развивается быстро, и каждый должен идти в ногу с изменениями[54].")]),t._v(" "),v("p",[v("strong",[t._v("Культура управления рисками.")]),t._v(" Нужно понимать риски, но не бояться их. Риск — это не причина отказываться от ИИ, это причина использовать его с осторожностью[54].")]),t._v(" "),v("p",[v("strong",[t._v("Культура человечности.")]),t._v(" ИИ — это инструмент для людей, а не замена людям. Человеческое суждение, креативность и эмоциональный интеллект остаются критически важными[54].")]),t._v(" "),v("p",[v("strong",[t._v("Культура инноваций.")]),t._v(" Но инновация должна быть этичной и ответственной[54].")]),t._v(" "),v("p",[t._v("На практике это означает[50]:")]),t._v(" "),v("p",[t._v("Во-первых, "),v("strong",[t._v("учитесь основам")]),t._v(". Прочитайте статьи, посмотрите видео, поговорите с людьми, которые знают. Не пытайтесь разбираться во всех сложностях, просто получите базовое понимание[50].")]),t._v(" "),v("p",[t._v("Во-вторых, "),v("strong",[t._v("экспериментируйте осторожно")]),t._v(". Попробуйте использовать ИИ в низкорисковых ситуациях. Спросите его о рецептах, попросите помощь в написании письма, поиграйте с ним. Это поможет вам развить интуицию о том, что он может[50].")]),t._v(" "),v("p",[t._v("В-третьих, "),v("strong",[t._v("знайте, когда ИИ не подходит")]),t._v(". Для высокорисковых решений (медицина, право, финансы) всегда проверяйте ИИ с человеком-экспертом[50].")]),t._v(" "),v("p",[t._v("В-четвертых, "),v("strong",[t._v("думайте критически")]),t._v(". Не верьте ИИ слепо. Задавайте вопросы. Если что-то звучит странно, проверьте это[50].")]),t._v(" "),v("h2",{attrs:{id:"специальные-рекомендации-для-человека-интроверта-и-экстраверта-в-доверии-к-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#специальные-рекомендации-для-человека-интроверта-и-экстраверта-в-доверии-к-ии"}},[t._v("#")]),t._v(" Специальные рекомендации для человека-интроверта и экстраверта в доверии к ИИ")]),t._v(" "),v("p",[t._v("Интересно, что демографические факторы влияют на доверие к ИИ[26]. Возраст, пол, знакомство с технологией — все это играет роль[26].")]),t._v(" "),v("p",[t._v("Люди, которые часто используют технологию, доверяют ИИ больше[26]. Пожилые люди часто более скептичны[26]. Люди с более высоким уровнем образования часто более скептичны (потому что они лучше понимают риски)[26].")]),t._v(" "),v("p",[t._v("Важно понимать, что "),v("strong",[t._v("нет правильного уровня доверия для всех")]),t._v("[26]. Некоторым людям нужно привыкать к ИИ медленнее, другим можно двигаться быстрее[26]. Главное — адаптировать свой подход к своему стилю обучения и удобству[26].")]),t._v(" "),v("h2",{attrs:{id:"несколько-конкретных-вопросов-которые-нужно-задавать-перед-использованием-ии"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#несколько-конкретных-вопросов-которые-нужно-задавать-перед-использованием-ии"}},[t._v("#")]),t._v(" Несколько конкретных вопросов, которые нужно задавать перед использованием ИИ")]),t._v(" "),v("p",[t._v("Перед тем, как полагаться на ИИ в важном решении, спросите себя[4][39][49]:")]),t._v(" "),v("p",[v("strong",[t._v("Во-первых:")]),t._v(" Было ли это протестировано? Как ИИ был обучен? На каких данных? Где доказательства, что он работает?")]),t._v(" "),v("p",[v("strong",[t._v("Во-вторых:")]),t._v(" Подходящие ли тренировочные данные? Похожи ли они на данные, которые я буду использовать? Если я использую ИИ на группе людей, на которых он не был обучен, результаты могут быть неправильными.")]),t._v(" "),v("p",[v("strong",[t._v("В-третьих:")]),t._v(" Могу ли я проверить результаты? Есть ли способ узнать, правильно ли ИИ работает?")]),t._v(" "),v("p",[v("strong",[t._v("В-четвертых:")]),t._v(" Кто несет ответственность, если ИИ ошибается? Что произойдет, если я буду следовать рекомендации ИИ и она окажется неправильной?")]),t._v(" "),v("p",[v("strong",[t._v("В-пятых:")]),t._v(" Есть ли человеческий надзор? Будет ли человек проверять результаты ИИ?")]),t._v(" "),v("p",[v("strong",[t._v("В-шестых:")]),t._v(" Есть ли способ остановить ИИ, если что-то пойдет не так?")]),t._v(" "),v("h2",{attrs:{id:"заключение-ии-это-инструмент-требующии-внимания"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#заключение-ии-это-инструмент-требующии-внимания"}},[t._v("#")]),t._v(" Заключение: ИИ — это инструмент, требующий внимания")]),t._v(" "),v("p",[t._v("На уровне 0 главный вывод состоит в том, что ИИ — это мощный, но несовершенный инструмент[1][4][49]. Его нельзя полностью игнорировать, потому что он становится все более важным. Но его также нельзя слепо доверять[1][4][49].")]),t._v(" "),v("p",[t._v("Правильный подход — это "),v("strong",[t._v("информированное скептицизм")]),t._v("[4][49]. Это означает, что вы готовы пробовать новые вещи, но вы также критичны. Вы понимаете возможности и ограничения. Вы знаете, когда использовать ИИ и когда полагаться на людей[4][49].")]),t._v(" "),v("p",[t._v("Только 46% людей в мире готовы доверять ИИ в 2025 году[5]. Это не значит, что эти люди правы, а остальные неправы. Это означает, что доверие к ИИ — это сложный вопрос, который каждый должен решить для себя[5][26].")]),t._v(" "),v("p",[t._v("Ваша задача как новичка — не паниковать и не восхищаться, а просто "),v("strong",[t._v("учиться")]),t._v("[50]. Сначала учитесь на основах. Потом экспериментируйте. Потом развивайте интуицию о том, что работает, а что нет. Это займет время, но это стоит того[50][54].")]),t._v(" "),v("p",[t._v("ИИ — это не враг, и это не спаситель. Это инструмент. И как любой инструмент, его можно использовать хорошо или плохо, в зависимости от того, кто его использует и как. Ваша работа — научиться использовать его хорошо[51][54].")]),t._v(" "),v("h2",{attrs:{id:"источники-инфы"}},[v("a",{staticClass:"header-anchor",attrs:{href:"#источники-инфы"}},[t._v("#")]),t._v(" Источники инфы")]),t._v(" "),v("ol",[v("li",[t._v("https://lifehacker.ru/doverie-iskusstvennomu-intellektu/")]),t._v(" "),v("li",[t._v("https://drainpipe.io/ai-trustworthiness-report-may-2025/")]),t._v(" "),v("li",[t._v("https://www.activemind.legal/guides/bias-ai/")]),t._v(" "),v("li",[t._v("https://xn--80aaaa1bcnbfmecccigb7k1ewa5b.xn--p1ai/2025/03/06/mozhno-li-doverjat-iskusstvennomu-intellektu/")]),t._v(" "),v("li",[t._v("https://kpmg.com/xx/en/our-insights/ai-and-technology/trust-attitudes-and-use-of-ai.html")]),t._v(" "),v("li",[t._v("https://www.holisticai.com/blog/what-is-ai-bias-risks-mitigation-strategies")]),t._v(" "),v("li",[t._v("https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)")]),t._v(" "),v("li",[t._v("https://www.joneswalker.com/en/insights/blogs/ai-law-blog/the-explainability-illusion-why-ai-transparency-requirements-miss-the-point.html?id=102kzle")]),t._v(" "),v("li",[t._v("https://professional.dce.harvard.edu/blog/building-a-responsible-ai-framework-5-key-principles-for-organizations/")]),t._v(" "),v("li",[t._v("https://mitsloanedtech.mit.edu/ai/basics/addressing-ai-hallucinations-and-bias/")]),t._v(" "),v("li",[t._v("https://www.f5.com/company/blog/crucial-concepts-in-ai-transparency-and-explainability")]),t._v(" "),v("li",[t._v("https://www.unesco.org/en/artificial-intelligence/recommendation-ethics")]),t._v(" "),v("li",[t._v("https://www.conductor.com/academy/ai-knowledge-cutoff/")]),t._v(" "),v("li",[t._v("https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence")]),t._v(" "),v("li",[t._v("https://artificialintelligenceact.eu/article/14/")]),t._v(" "),v("li",[t._v("https://www.promptlayer.com/glossary/knowledge-cutoff")]),t._v(" "),v("li",[t._v("https://artificialintelligenceact.eu")]),t._v(" "),v("li",[t._v("https://fayedigital.com/blog/ai-governance-framework/")]),t._v(" "),v("li",[t._v("https://www.tableau.com/data-insights/ai/advantages-disadvantages")]),t._v(" "),v("li",[t._v("https://www.leewayhertz.com/build-recommendation-system/")]),t._v(" "),v("li",[t._v("https://www.ibm.com/think/insights/10-ai-dangers-and-risks-and-how-to-manage-them")]),t._v(" "),v("li",[t._v("https://www.online.uc.edu/blog/artificial-intelligence-ai-benefits")]),t._v(" "),v("li",[t._v("https://www.nvidia.com/en-us/glossary/recommendation-system/")]),t._v(" "),v("li",[t._v("https://safe.ai/ai-risk")]),t._v(" "),v("li",[t._v("https://pubsonline.informs.org/do/10.1287/LYTX.2024.04.09/full/")]),t._v(" "),v("li",[t._v("https://pmc.ncbi.nlm.nih.gov/articles/PMC12588509/")]),t._v(" "),v("li",[t._v("https://hai.stanford.edu/policy/validating-claims-about-ai-a-policymakers-guide")]),t._v(" "),v("li",[t._v("https://birchwoodu.org/the-ultimate-guide-to-domain-specific-ai/")]),t._v(" "),v("li",[t._v("https://www.nature.com/articles/s41599-024-04044-8")]),t._v(" "),v("li",[t._v("https://digital-skills-jobs.europa.eu/en/latest/briefs/artificial-intelligence-high-stakes-game-what-cost-deep-dive")]),t._v(" "),v("li",[t._v("https://www.6clicks.com/resources/blog/responsible-ai-best-practices-real-world-examples")]),t._v(" "),v("li",[t._v("https://www.articulate.com/blog/how-to-fact-check-ai-content-like-a-pro/")]),t._v(" "),v("li",[t._v("https://partnershiponai.org/paper/human-ai-collaboration-framework-case-studies/")]),t._v(" "),v("li",[t._v("https://hbr.org/2024/05/how-to-implement-ai-responsibly")]),t._v(" "),v("li",[t._v("https://guides.library.vcu.edu/ai/factcheck")]),t._v(" "),v("li",[t._v("http://partnershiponai.org/wp-content/uploads/2021/08/CPAIS-Framework-and-Case-Studies-9-23.pdf")]),t._v(" "),v("li",[t._v("https://mitsloan.mit.edu/ideas-made-to-matter/a-framework-assessing-ai-risk")]),t._v(" "),v("li",[t._v("https://community.deeplearning.ai/t/deciding-when-to-use-a-decision-tree-model/214747")]),t._v(" "),v("li",[t._v("https://pmc.ncbi.nlm.nih.gov/articles/PMC12340025/")]),t._v(" "),v("li",[t._v("https://www.nist.gov/itl/ai-risk-management-framework")]),t._v(" "),v("li",[t._v("https://www.ibm.com/think/topics/decision-trees")]),t._v(" "),v("li",[t._v("https://cio.ucsb.edu/artificial-intelligence/ai-use-guidelines")]),t._v(" "),v("li",[t._v("https://www.iso.org/artificial-intelligence/natural-language-processing")]),t._v(" "),v("li",[t._v("https://hub.jhu.edu/2025/09/02/multilingual-artificial-intelligence-often-reinforces-bias/")]),t._v(" "),v("li",[t._v("https://www.jbs.cam.ac.uk/2025/human-brain-vs-ai-what-makes-better-decisions/")]),t._v(" "),v("li",[t._v("https://sam-solutions.com/blog/natural-language-understanding-in-ai/")]),t._v(" "),v("li",[t._v("https://academic.oup.com/pnasnexus/article/3/9/pgae346/7756548")]),t._v(" "),v("li",[t._v("https://mitsloan.mit.edu/ideas-made-to-matter/when-humans-and-ai-work-best-together-and-when-each-better-alone")]),t._v(" "),v("li",[t._v("https://pmc.ncbi.nlm.nih.gov/articles/PMC8714273/")]),t._v(" "),v("li",[t._v("https://cedr.gatech.edu/ai-literacy-government/")]),t._v(" "),v("li",[t._v("https://www.indwes.edu/articles/2025/06/build-a-culture-of-responsible-ai")]),t._v(" "),v("li",[t._v("https://www.ischool.berkeley.edu/sites/default/files/sproject_attachments/humanai_capstonereport-final.pdf")]),t._v(" "),v("li",[t._v("https://digitalpromise.org/2024/06/18/ai-literacy-a-framework-to-understand-evaluate-and-use-emerging-technology/")]),t._v(" "),v("li",[t._v("https://www.wtwco.com/en-sa/insights/2025/11/how-company-culture-can-drive-effective-ai-adoption")]),t._v(" "),v("li",[t._v("https://www.nature.com/articles/s41599-024-04044-8")]),t._v(" "),v("li",[t._v("https://arxiv.org/html/2404.14082v1")]),t._v(" "),v("li",[t._v("https://www.ntia.gov/issues/artificial-intelligence/ai-accountability-policy-report/overview")]),t._v(" "),v("li",[t._v("https://www.colorado.edu/today/2025/10/21/5-ways-make-ai-more-trustworthy")]),t._v(" "),v("li",[t._v("https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/")]),t._v(" "),v("li",[t._v("https://www.ibm.com/think/topics/ai-governance")])])])}),[],!1,null,null,null);v.default=_.exports}}]);
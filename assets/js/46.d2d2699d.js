(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{369:function(t,a,r){"use strict";r.r(a);var _=r(18),v=Object(_.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"attention-механизм-простыми-словами"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#attention-механизм-простыми-словами"}},[t._v("#")]),t._v(" Attention механизм простыми словами")]),t._v(" "),a("p",[a("strong",[t._v("Attention")]),t._v(' (механизм внимания) — это ключевая технология, которая позволяет моделям типа GPT "обращать внимание" на разные части входного текста при генерации каждого слова ответа.')]),t._v(" "),a("h2",{attrs:{id:"интуитивное-понимание"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#интуитивное-понимание"}},[t._v("#")]),t._v(" Интуитивное понимание")]),t._v(" "),a("p",[t._v("Представь, что ты переводишь предложение с русского на английский. При переводе каждого слова ты смотришь на разные части исходного предложения:")]),t._v(" "),a("ul",[a("li",[t._v('Для перевода "кошка" смотришь на "cat" в оригинале')]),t._v(" "),a("li",[t._v('Для перевода "сидит" смотришь на "sits"')]),t._v(" "),a("li",[t._v("Для правильного порядка слов смотришь на всю структуру предложения")])]),t._v(" "),a("p",[t._v('Attention работает похоже: при генерации каждого слова модель "смотрит" на разные части входного текста и решает, на что обратить больше внимания.')]),t._v(" "),a("h2",{attrs:{id:"как-это-работает"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#как-это-работает"}},[t._v("#")]),t._v(" Как это работает?")]),t._v(" "),a("h3",{attrs:{id:"self-attention-самовнимание"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#self-attention-самовнимание"}},[t._v("#")]),t._v(" Self-Attention (самовнимание)")]),t._v(" "),a("p",[t._v("Модель анализирует, как слова в тексте связаны друг с другом:")]),t._v(" "),a("ul",[a("li",[t._v('"Он" может относиться к "Иван" из начала предложения')]),t._v(" "),a("li",[t._v('"Его" может относиться к упомянутому ранее объекту')])]),t._v(" "),a("h3",{attrs:{id:"cross-attention-перекрестное-внимание"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cross-attention-перекрестное-внимание"}},[t._v("#")]),t._v(" Cross-Attention (перекрёстное внимание)")]),t._v(" "),a("p",[t._v('В задачах типа "вопрос-ответ" модель обращает внимание на разные части вопроса при генерации ответа.')]),t._v(" "),a("h2",{attrs:{id:"почему-это-важно"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#почему-это-важно"}},[t._v("#")]),t._v(" Почему это важно?")]),t._v(" "),a("p",[t._v("Без attention модели не могли:")]),t._v(" "),a("ul",[a("li",[t._v("Понимать длинные зависимости (связь между словами далеко друг от друга)")]),t._v(" "),a("li",[t._v("Правильно обрабатывать контекст")]),t._v(" "),a("li",[t._v("Генерировать связные длинные тексты")])]),t._v(" "),a("p",[t._v("С attention модели могут:")]),t._v(" "),a("ul",[a("li",[t._v('"Помнить" важную информацию из начала текста')]),t._v(" "),a("li",[t._v("Понимать связи между далёкими частями текста")]),t._v(" "),a("li",[t._v("Генерировать более связные и релевантные ответы")])]),t._v(" "),a("h2",{attrs:{id:"технические-детали-упрощенно"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#технические-детали-упрощенно"}},[t._v("#")]),t._v(" Технические детали (упрощённо)")]),t._v(" "),a("p",[t._v('Attention вычисляет "веса внимания" — числа, показывающие, насколько важно каждое слово входного текста для генерации текущего слова:')]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("Query (запрос)")]),t._v(': "Что я ищу?"')]),t._v(" "),a("li",[a("strong",[t._v("Key (ключ)")]),t._v(': "Что предлагает каждое слово?"')]),t._v(" "),a("li",[a("strong",[t._v("Value (значение)")]),t._v(': "Что содержит каждое слово?"')])]),t._v(" "),a("p",[t._v("Модель сравнивает Query с каждым Key и решает, на какие Value обратить больше внимания.")]),t._v(" "),a("h2",{attrs:{id:"multi-head-attention"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#multi-head-attention"}},[t._v("#")]),t._v(" Multi-Head Attention")]),t._v(" "),a("p",[t._v('Современные модели используют несколько "голов" внимания одновременно:')]),t._v(" "),a("ul",[a("li",[t._v("Каждая голова обращает внимание на разные аспекты")]),t._v(" "),a("li",[t._v("Результаты объединяются для более полного понимания")])]),t._v(" "),a("p",[t._v("Это как смотреть на картину разными способами: один раз обращаешь внимание на цвета, другой — на композицию, третий — на детали.")]),t._v(" "),a("h2",{attrs:{id:"transformer-архитектура"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformer-архитектура"}},[t._v("#")]),t._v(" Transformer архитектура")]),t._v(" "),a("p",[t._v("Attention — основа архитектуры Transformer, которая используется в GPT, BERT и других современных моделях:")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Encoder")]),t._v(": обрабатывает входной текст с помощью attention")]),t._v(" "),a("li",[a("strong",[t._v("Decoder")]),t._v(": генерирует ответ, обращая внимание на вход и уже сгенерированную часть")])]),t._v(" "),a("h2",{attrs:{id:"практическое-значение"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#практическое-значение"}},[t._v("#")]),t._v(" Практическое значение")]),t._v(" "),a("p",[t._v("Благодаря attention:")]),t._v(" "),a("ul",[a("li",[t._v("ChatGPT понимает контекст всего разговора")]),t._v(" "),a("li",[t._v("Модели переводов учитывают весь контекст предложения")]),t._v(" "),a("li",[t._v("Модели могут работать с длинными документами")])]),t._v(" "),a("h2",{attrs:{id:"ограничения"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ограничения"}},[t._v("#")]),t._v(" Ограничения")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Вычислительная сложность")]),t._v(": растёт квадратично с длиной текста")]),t._v(" "),a("li",[a("strong",[t._v("Контекстное окно")]),t._v(": всё равно ограничено (хотя и большое)")]),t._v(" "),a("li",[a("strong",[t._v("Интерпретируемость")]),t._v(": сложно понять, почему модель обратила внимание именно на эти слова")])]),t._v(" "),a("h2",{attrs:{id:"уровень"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#уровень"}},[t._v("#")]),t._v(" Уровень")]),t._v(" "),a("p",[t._v("3")]),t._v(" "),a("h2",{attrs:{id:"примеры-из-жизни"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#примеры-из-жизни"}},[t._v("#")]),t._v(" Примеры из жизни")]),t._v(" "),a("ul",[a("li",[t._v("ChatGPT использует attention для понимания контекста всего разговора")]),t._v(" "),a("li",[t._v("Переводчик учитывает весь контекст предложения благодаря attention")]),t._v(" "),a("li",[t._v("Модели для анализа документов находят связи между разными частями текста")])]),t._v(" "),a("h2",{attrs:{id:"полезные-ссылки-для-этого-уровня"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#полезные-ссылки-для-этого-уровня"}},[t._v("#")]),t._v(" Полезные ссылки для этого уровня")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://arxiv.org/abs/1706.03762",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention Is All You Need (оригинальная статья)"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Illustrated Transformer"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Understanding Attention"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"заметки"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#заметки"}},[t._v("#")]),t._v(" Заметки")])])}),[],!1,null,null,null);a.default=v.exports}}]);
# LoRA: быстрая адаптация моделей

**LoRA** (Low-Rank Adaptation) — это техника эффективного дообучения больших моделей, которая позволяет адаптировать модель под свои задачи, обучая только небольшую часть параметров вместо всей модели.

## Проблема, которую решает LoRA

Обычный fine-tuning требует:
- Много вычислительных ресурсов (GPU, время)
- Много памяти для хранения всех весов модели
- Долгое время обучения

LoRA решает это, обучая только небольшие дополнительные матрицы, которые "адаптируют" оригинальную модель.

## Как работает LoRA?

Вместо изменения всех весов модели, LoRA:
1. Добавляет небольшие матрицы (low-rank matrices) к некоторым слоям
2. Обучает только эти новые матрицы
3. При использовании комбинирует оригинальные веса с обученными матрицами

Результат: модель работает как дообученная, но:
- Обучается в 10-100 раз быстрее
- Требует в 10-100 раз меньше памяти
- Можно хранить много разных адаптаций одной модели

## Преимущества LoRA

✅ **Быстрое обучение**: часы вместо дней
✅ **Экономия памяти**: можно обучать на обычных GPU
✅ **Модульность**: можно переключаться между разными адаптациями
✅ **Эффективность**: результаты близки к полному fine-tuning

## Когда использовать LoRA?

✅ Нужно быстро протестировать разные адаптации
✅ Ограниченные вычислительные ресурсы
✅ Нужно несколько специализированных версий модели
✅ Экспериментирование с разными задачами

❌ Нужна максимальная точность (полный fine-tuning может быть лучше)
❌ Очень специфические задачи, требующие глубоких изменений

## Технические детали

### Rank (ранг)
Параметр, который определяет размер адаптационных матриц:
- Низкий rank (1-4): быстрее, меньше памяти, но может быть менее точным
- Высокий rank (16-64): медленнее, больше памяти, но точнее

### Alpha
Параметр масштабирования, влияет на силу адаптации.

### Target modules
Выбор слоёв для адаптации (обычно attention layers).

## Популярные библиотеки

### PEFT (Hugging Face)
```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,  # rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

model = get_peft_model(base_model, config)
```

### LoRA для разных моделей
- Llama, Mistral: хорошо работают с LoRA
- GPT-модели: можно использовать через специальные библиотеки
- Stable Diffusion: популярно для генерации изображений

## Пример workflow

1. **Выбор базовой модели**: например, Llama 2 7B
2. **Подготовка данных**: специфические примеры для задачи
3. **Настройка LoRA**: выбор rank, alpha, target modules
4. **Обучение**: обычно несколько часов на одной GPU
5. **Тестирование**: проверка на валидационных данных
6. **Использование**: загрузка базовой модели + LoRA веса

## Сравнение с fine-tuning

| Параметр | Fine-tuning | LoRA |
|----------|-------------|------|
| Скорость обучения | Дни | Часы |
| Память | Вся модель | Небольшие матрицы |
| Точность | Высокая | Очень близкая |
| Гибкость | Одна версия | Много адаптаций |

## Ограничения

- Может быть немного менее точным, чем полный fine-tuning
- Нужно понимать параметры (rank, alpha)
- Не все модели одинаково хорошо работают с LoRA

## Уровень
3

## Примеры из жизни
- Быстрое создание специализированных версий модели для разных задач
- Адаптация модели под стиль компании
- Экспериментирование с разными доменами (медицина, право, код)
- Создание персонализированных моделей для разных пользователей

## Полезные ссылки для этого уровня
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [LoRA для Stable Diffusion](https://github.com/bmaltais/kohya_ss)

## Заметки

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Искусственный интеллект: Полный курс для студентов | ИИ от нуля до профи</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/aikb/favicon.ico">
    <meta name="description" content="Самая понятная русскоязычная база знаний об искусственном интеллекте, нейросетях и агентах">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link rel="preload" href="/aikb/assets/css/0.styles.cb552d59.css" as="style"><link rel="preload" href="/aikb/assets/js/app.7ae53dfc.js" as="script"><link rel="preload" href="/aikb/assets/js/2.7772ee18.js" as="script"><link rel="preload" href="/aikb/assets/js/1.64231b73.js" as="script"><link rel="preload" href="/aikb/assets/js/40.9f2b4cd6.js" as="script"><link rel="prefetch" href="/aikb/assets/js/10.76f92d53.js"><link rel="prefetch" href="/aikb/assets/js/11.f7af14d6.js"><link rel="prefetch" href="/aikb/assets/js/12.e0ace64d.js"><link rel="prefetch" href="/aikb/assets/js/13.55e74aa0.js"><link rel="prefetch" href="/aikb/assets/js/14.3a60391f.js"><link rel="prefetch" href="/aikb/assets/js/15.9ed06451.js"><link rel="prefetch" href="/aikb/assets/js/16.31eafacf.js"><link rel="prefetch" href="/aikb/assets/js/17.4802f85b.js"><link rel="prefetch" href="/aikb/assets/js/18.088f02af.js"><link rel="prefetch" href="/aikb/assets/js/19.5cb15882.js"><link rel="prefetch" href="/aikb/assets/js/20.71d27f6a.js"><link rel="prefetch" href="/aikb/assets/js/21.2abc8d57.js"><link rel="prefetch" href="/aikb/assets/js/22.70abbdfb.js"><link rel="prefetch" href="/aikb/assets/js/23.8d6d2b76.js"><link rel="prefetch" href="/aikb/assets/js/24.3a9890a3.js"><link rel="prefetch" href="/aikb/assets/js/25.669f2e66.js"><link rel="prefetch" href="/aikb/assets/js/26.f94c6f17.js"><link rel="prefetch" href="/aikb/assets/js/27.b30bdc5e.js"><link rel="prefetch" href="/aikb/assets/js/28.17d51787.js"><link rel="prefetch" href="/aikb/assets/js/29.69c70364.js"><link rel="prefetch" href="/aikb/assets/js/3.48060444.js"><link rel="prefetch" href="/aikb/assets/js/30.7a8003c3.js"><link rel="prefetch" href="/aikb/assets/js/31.de5215a5.js"><link rel="prefetch" href="/aikb/assets/js/32.6d4df069.js"><link rel="prefetch" href="/aikb/assets/js/33.9a66fc15.js"><link rel="prefetch" href="/aikb/assets/js/34.36344a1a.js"><link rel="prefetch" href="/aikb/assets/js/35.b591d06b.js"><link rel="prefetch" href="/aikb/assets/js/36.9faa30e7.js"><link rel="prefetch" href="/aikb/assets/js/37.4a1484a4.js"><link rel="prefetch" href="/aikb/assets/js/38.5b0f9649.js"><link rel="prefetch" href="/aikb/assets/js/39.1f8f47d7.js"><link rel="prefetch" href="/aikb/assets/js/4.54bb16ac.js"><link rel="prefetch" href="/aikb/assets/js/41.238d0487.js"><link rel="prefetch" href="/aikb/assets/js/42.f391c83a.js"><link rel="prefetch" href="/aikb/assets/js/43.b0deb88f.js"><link rel="prefetch" href="/aikb/assets/js/44.46b3f282.js"><link rel="prefetch" href="/aikb/assets/js/45.7ec6fcf2.js"><link rel="prefetch" href="/aikb/assets/js/46.51449979.js"><link rel="prefetch" href="/aikb/assets/js/47.973848ed.js"><link rel="prefetch" href="/aikb/assets/js/48.7ed93910.js"><link rel="prefetch" href="/aikb/assets/js/49.f26dcbc4.js"><link rel="prefetch" href="/aikb/assets/js/5.c963c83c.js"><link rel="prefetch" href="/aikb/assets/js/50.1f27de14.js"><link rel="prefetch" href="/aikb/assets/js/51.51c31c45.js"><link rel="prefetch" href="/aikb/assets/js/52.d8da2a41.js"><link rel="prefetch" href="/aikb/assets/js/53.c19bcd23.js"><link rel="prefetch" href="/aikb/assets/js/54.b067f63b.js"><link rel="prefetch" href="/aikb/assets/js/55.59abc7d9.js"><link rel="prefetch" href="/aikb/assets/js/56.e37b3faa.js"><link rel="prefetch" href="/aikb/assets/js/57.5b1c0405.js"><link rel="prefetch" href="/aikb/assets/js/58.da9e93d9.js"><link rel="prefetch" href="/aikb/assets/js/59.e81af16e.js"><link rel="prefetch" href="/aikb/assets/js/6.d6e7d6f1.js"><link rel="prefetch" href="/aikb/assets/js/60.1cb46976.js"><link rel="prefetch" href="/aikb/assets/js/61.c68e5a89.js"><link rel="prefetch" href="/aikb/assets/js/62.7f7ff9d9.js"><link rel="prefetch" href="/aikb/assets/js/63.50a8943d.js"><link rel="prefetch" href="/aikb/assets/js/64.9d957642.js"><link rel="prefetch" href="/aikb/assets/js/65.f74b5b07.js"><link rel="prefetch" href="/aikb/assets/js/66.e01555d6.js"><link rel="prefetch" href="/aikb/assets/js/67.b8f70556.js"><link rel="prefetch" href="/aikb/assets/js/68.113e8931.js"><link rel="prefetch" href="/aikb/assets/js/69.b134eba5.js"><link rel="prefetch" href="/aikb/assets/js/7.95f8db1e.js"><link rel="prefetch" href="/aikb/assets/js/70.b820c69a.js"><link rel="prefetch" href="/aikb/assets/js/71.286d2b63.js"><link rel="prefetch" href="/aikb/assets/js/72.6af06eaf.js"><link rel="prefetch" href="/aikb/assets/js/73.674e29f5.js"><link rel="prefetch" href="/aikb/assets/js/74.61ef6eef.js"><link rel="prefetch" href="/aikb/assets/js/75.6d7362d4.js"><link rel="prefetch" href="/aikb/assets/js/76.a0441a88.js"><link rel="prefetch" href="/aikb/assets/js/77.fb8aeb60.js"><link rel="prefetch" href="/aikb/assets/js/78.ad241bc6.js"><link rel="prefetch" href="/aikb/assets/js/79.798892de.js"><link rel="prefetch" href="/aikb/assets/js/80.865a337a.js"><link rel="prefetch" href="/aikb/assets/js/81.28f29d9e.js"><link rel="prefetch" href="/aikb/assets/js/82.19c469da.js"><link rel="prefetch" href="/aikb/assets/js/83.0f3aeb85.js"><link rel="prefetch" href="/aikb/assets/js/84.14f50359.js"><link rel="prefetch" href="/aikb/assets/js/85.d3664232.js"><link rel="prefetch" href="/aikb/assets/js/86.13788c92.js"><link rel="prefetch" href="/aikb/assets/js/87.57b2c3fa.js"><link rel="prefetch" href="/aikb/assets/js/vendors~docsearch.283a746b.js">
    <link rel="stylesheet" href="/aikb/assets/css/0.styles.cb552d59.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/aikb/" class="home-link router-link-active"><img src="/aikb/logo.png" alt="ИИ от нуля до профи" class="logo"> <span class="site-name can-hide">ИИ от нуля до профи</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/aikb/" class="nav-link">
  Главная
</a></div><div class="nav-item"><a href="/aikb/index.html" class="nav-link">
  База знаний
</a></div><div class="nav-item"><a href="https://github.com/IgorKramar/aikb" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/aikb/" class="nav-link">
  Главная
</a></div><div class="nav-item"><a href="/aikb/index.html" class="nav-link">
  База знаний
</a></div><div class="nav-item"><a href="https://github.com/IgorKramar/aikb" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Основы (Уровень 0)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Промптинг (Уровень 1-2)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Практика (Уровень 1-2)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Технические основы (Уровень 1-2)</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Продвинутые темы (Уровень 3)</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/aikb/topics/ai-full-academic.html" aria-current="page" class="active sidebar-link">Искусственный интеллект: Полный курс для студентов</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#введение" class="sidebar-link">Введение</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-1-история-ии-и-философские-основания" class="sidebar-link">Модуль 1: История ИИ и философские основания</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-2-фундаментальные-концепции-машинного-обучения" class="sidebar-link">Модуль 2: Фундаментальные концепции машинного обучения</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-3-математические-основы-неиросетеи" class="sidebar-link">Модуль 3: Математические основы нейросетей</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-4-обучение-неиросетеи" class="sidebar-link">Модуль 4: Обучение нейросетей</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-5-проблемы-обучения-и-их-решения" class="sidebar-link">Модуль 5: Проблемы обучения и их решения</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-6-современные-архитектуры-2025" class="sidebar-link">Модуль 6: Современные архитектуры (2025)</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-7-актуальное-состояние-ии-в-2025-году" class="sidebar-link">Модуль 7: Актуальное состояние ИИ в 2025 году</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#модуль-8-практическое-применение-и-развитие-навыков" class="sidebar-link">Модуль 8: Практическое применение и развитие навыков</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#заключение-куда-дальше" class="sidebar-link">Заключение: Куда дальше?</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#рекомендации-для-дальнеишего-изучения" class="sidebar-link">Рекомендации для дальнейшего изучения</a></li><li class="sidebar-sub-header"><a href="/aikb/topics/ai-full-academic.html#сноски" class="sidebar-link">Сноски</a></li></ul></li><li><a href="/aikb/topics/embeddings.html" class="sidebar-link">Эмбеддинги: как ИИ понимает смысл</a></li><li><a href="/aikb/topics/vector-databases.html" class="sidebar-link">Векторные базы данных</a></li><li><a href="/aikb/topics/fine-tuning.html" class="sidebar-link">Fine-tuning: обучение модели под свои задачи</a></li><li><a href="/aikb/topics/lora.html" class="sidebar-link">LoRA: быстрая адаптация моделей</a></li><li><a href="/aikb/topics/lora-creation-guide.html" class="sidebar-link">Создание LoRA: пошаговый практический гайд</a></li><li><a href="/aikb/topics/attention-mechanism.html" class="sidebar-link">Attention механизм простыми словами</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Инструменты</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="искусственныи-интеллект-полныи-курс-для-студентов"><a href="#искусственныи-интеллект-полныи-курс-для-студентов" class="header-anchor">#</a> Искусственный интеллект: Полный курс для студентов</h1> <h2 id="введение"><a href="#введение" class="header-anchor">#</a> Введение</h2> <p>Добро пожаловать на курс по искусственному интеллекту. Эта лекция представляет собой расширенное введение в одну из наиболее трансформирующих технологий нашего времени. Мы начнём с исторического контекста, рассмотрим фундаментальные концепции, углубимся в математику, которая лежит в основе современных систем, и проанализируем актуальное состояние ИИ в 2025 году.</p> <p><strong>Цель курса</strong>: выработать у студентов глубокое понимание того, как работает искусственный интеллект, не просто на уровне аналогий, но на уровне математических и алгоритмических принципов, которые его движут.</p> <h2 id="модуль-1-история-ии-и-философские-основания"><a href="#модуль-1-история-ии-и-философские-основания" class="header-anchor">#</a> Модуль 1: История ИИ и философские основания</h2> <h3 id="_1-1-от-идеи-к-дисциплине-алан-тьюринг-и-его-наследие"><a href="#_1-1-от-идеи-к-дисциплине-алан-тьюринг-и-его-наследие" class="header-anchor">#</a> 1.1 От идеи к дисциплине: Алан Тьюринг и его наследие</h3> <p>История современного ИИ начинается с одного человека и одного вопроса. В 1950 году британский математик <strong>Алан Тьюринг</strong> опубликовал революционную статью &quot;Computing Machinery and Intelligence&quot; (&quot;Вычислительные машины и разум&quot;), где сформулировал фундаментальный вопрос: <strong>&quot;Может ли машина думать?&quot;</strong>.[1][2][3]</p> <p>Тьюринг понимал, что попытка прямого ответа на этот вопрос философски сложна. Вместо этого он предложил <strong>тест Тьюринга</strong> — практический способ определить, может ли машина имитировать человеческий интеллект достаточно хорошо, чтобы эксперт не смог отличить её от реального человека. Закрытый за экраном человек задаёт вопросы двум собеседникам: одному человеку и одной машине. Если машина неотличима от человека, Тьюринг считал, что её справедливо назвать &quot;думающей&quot;.[4][1]</p> <p><strong>Почему это важно?</strong> Тьюринг сделал несколько критических наблюдений:[1]</p> <ol><li><strong>Машины способны обучаться</strong> — они не должны быть просто запрограммированы на выполнение команд</li> <li><strong>Интеллект — это относительная концепция</strong> — если машина неотличима в поведении, нет смысла спорить, &quot;действительно ли она думает&quot;</li> <li><strong>Этические вопросы</strong> — развитие машин, которые мыслят, будет иметь глубокие социальные и философские последствия</li></ol> <p>Тьюринг был не просто инженером, он был философом, размышляющим о природе сознания и месте машин в обществе.</p> <h3 id="_1-2-рождение-ии-как-научнои-дисциплины-дартмут-1956"><a href="#_1-2-рождение-ии-как-научнои-дисциплины-дартмут-1956" class="header-anchor">#</a> 1.2 Рождение ИИ как научной дисциплины: Дартмут, 1956</h3> <p>Хотя идеи носились в воздухе, официальным рождением ИИ как отдельной научной дисциплины считается <strong>Дартмутский летний семинар 1956 года</strong>.[2][3][1]</p> <p>На эту конференцию собрались ведущие умы того времени:[1]</p> <ul><li><strong>John McCarthy</strong> — изобрёл LISP (первый язык для ИИ)</li> <li><strong>Marvin Minsky</strong> — пионер нейросетей</li> <li><strong>Claude Shannon</strong> — основатель теории информации</li> <li><strong>Nathaniel Rochester</strong> — инженер IBM</li> <li>Другие выдающиеся учёные</li></ul> <p>На этом семинаре участники поставили амбициозную цель: <strong>создать разумную машину за несколько месяцев</strong>. Они были ошеломлены, насколько это оказалось сложнее, чем ожидалось. Однако, конференция установила основные парадигмы исследований, которые определили развитие поля на десятилетия вперёд:</p> <ol><li><strong>Символический подход</strong> — интеллект работает с символами и логическими правилами</li> <li><strong>Поиск в пространстве состояний</strong> — решение задач через систематический поиск</li> <li><strong>Знаниевые системы</strong> — накопление человеческого знания в базах данных</li></ol> <h3 id="_1-3-зимы-ии-и-периоды-разочарования"><a href="#_1-3-зимы-ии-и-периоды-разочарования" class="header-anchor">#</a> 1.3 Зимы ИИ и периоды разочарования</h3> <p>История ИИ не была прямолинейной. После оптимизма 1950-60х годов пришли периоды скептицизма:[4][1]</p> <p><strong>Первая зима ИИ (1974-1980)</strong>:</p> <ul><li>Ранние системы попались в &quot;проклятие размерности&quot; — задачи, казавшиеся простыми для людей, требовали экспоненциального увеличения вычислительных ресурсов</li> <li>Символические системы не могли справиться с реальной сложностью</li> <li>Финансирование была урезано</li></ul> <p><strong>Вторая зима ИИ (1987-1993)</strong>:</p> <ul><li>Узкоспециализированные &quot;экспертные системы&quot; потеряли популярность</li> <li>Вычислительные мощности были недостаточны</li> <li>Ожидания были нереалистичны</li></ul> <p>Однако, каждый раз, когда поле казалось мёртвым, появлялись новые идеи и технологии, которые возрождали его.</p> <h3 id="_1-4-философские-вопросы-которые-остаются-открытыми"><a href="#_1-4-философские-вопросы-которые-остаются-открытыми" class="header-anchor">#</a> 1.4 Философские вопросы, которые остаются открытыми</h3> <p>Несмотря на технический прогресс, философские вопросы остаются нерешёнными:[5][6]</p> <table><thead><tr><th>Вопрос</th> <th>Статус</th></tr></thead> <tbody><tr><td><strong>Может ли машина &quot;действительно&quot; думать?</strong></td> <td>Открыт — зависит от определения &quot;думать&quot;</td></tr> <tr><td><strong>Имеет ли ИИ сознание?</strong></td> <td>Нет — текущие системы это просто математические функции</td></tr> <tr><td><strong>Может ли ИИ иметь опыт (qualia)?</strong></td> <td>Неизвестно — это философский вопрос о природе опыта</td></tr> <tr><td><strong>Имеет ли ИИ намерения?</strong></td> <td>Нет — только того намерения, которое мы вложили</td></tr></tbody></table> <p><strong>Важное различие</strong>, которое должны понимать студенты: между <strong>&quot;функциональным&quot;</strong> и <strong>&quot;феноменологическим&quot;</strong> пониманием интеллекта.</p> <ul><li><strong>Функциональный подход</strong> (Тьюринг): если это выглядит и действует как интеллект, то это интеллект</li> <li><strong>Феноменологический подход</strong>: может ли машина действительно испытывать что-то? Есть ли &quot;что это такое&quot; — быть ИИ?</li></ul> <p>На данный момент нет научного способа ответить на феноменологический вопрос.</p> <h2 id="модуль-2-фундаментальные-концепции-машинного-обучения"><a href="#модуль-2-фундаментальные-концепции-машинного-обучения" class="header-anchor">#</a> Модуль 2: Фундаментальные концепции машинного обучения</h2> <h3 id="_2-1-трехуровневая-иерархия-ии"><a href="#_2-1-трехуровневая-иерархия-ии" class="header-anchor">#</a> 2.1 Трёхуровневая иерархия ИИ</h3> <p>Существует важное различие между тремя взаимосвязанными, но разными областями:[6][5]</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>┌─────────────────────────────────────────────┐
│  Искусственный интеллект (ИИ)              │
│  Любая система, демонстрирующая            │
│  &quot;интеллектуальное&quot; поведение              │
└─────────────────────────────────────────────┘
                      ▲
                      │
┌─────────────────────────────────────────────┐
│  Машинное обучение (ML)                     │
│  Системы, которые улучшаются                │
│  на основе опыта/данных                     │
└─────────────────────────────────────────────┘
                      ▲
                      │
┌─────────────────────────────────────────────┐
│  Глубокое обучение (Deep Learning)          │
│  Нейросети с многими слоями                │
│  для обработки сложных данных              │
└─────────────────────────────────────────────┘
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p><strong>Не все ИИ использует обучение</strong> — система правил (экспертная система) также ИИ, но не машинное обучение. <strong>Не все машинное обучение — глубокое обучение</strong> — деревья решений, SVM и другие методы тоже ML, но не используют нейросети.</p> <h3 id="_2-2-формальное-определение-машинного-обучения"><a href="#_2-2-формальное-определение-машинного-обучения" class="header-anchor">#</a> 2.2 Формальное определение машинного обучения</h3> <p>Американский компьютерный учёный <strong>Том Митчелл</strong> дал классическое определение, которое используется и сегодня:</p> <p><strong>&quot;Компьютерная программа считается обучаемой опыту E в отношении класса задач T и меры производительности P, если её производительность по задачам в T, измеренная в P, улучшается с опытом E&quot;</strong></p> <p>Распакуем это:</p> <ul><li><strong>E (Experience)</strong> — данные, которые видит система</li> <li><strong>T (Task)</strong> — конкретная задача, которую нужно решить</li> <li><strong>P (Performance)</strong> — как мы измеряем успех</li></ul> <p>Пример: система распознавания спама</p> <ul><li>E = миллионы примеров email, помеченные как &quot;спам&quot; или &quot;не спам&quot;</li> <li>T = классификация новых email как спам или нет</li> <li>P = процент правильно классифицированных писем</li></ul> <h3 id="_2-3-основные-типы-машинного-обучения"><a href="#_2-3-основные-типы-машинного-обучения" class="header-anchor">#</a> 2.3 Основные типы машинного обучения</h3> <h4 id="обучение-с-учителем-supervised-learning"><a href="#обучение-с-учителем-supervised-learning" class="header-anchor">#</a> Обучение с учителем (Supervised Learning)</h4> <p>При этом подходе система обучается на <strong>размеченных примерах</strong> — каждому входу соответствует известный правильный ответ.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Обучающие данные:
┌─────────────┬─────────────┐
│   Вход      │   Выход     │
├─────────────┼─────────────┤
│ [картинка]  │   &quot;кот&quot;     │
│ [картинка]  │   &quot;собака&quot;  │
│ [картинка]  │   &quot;кот&quot;     │
└─────────────┴─────────────┘

Задача: предсказать класс новой картинки
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p><strong>Два подтипа:</strong></p> <ol><li><strong>Классификация</strong> — предсказать категорию (спам/не спам, кот/собака/птица)</li> <li><strong>Регрессия</strong> — предсказать числовое значение (цена дома, температура завтра)</li></ol> <p>Пример алгоритмов: логистическая регрессия, SVM, нейросети, случайные леса.</p> <h4 id="обучение-без-учителя-unsupervised-learning"><a href="#обучение-без-учителя-unsupervised-learning" class="header-anchor">#</a> Обучение без учителя (Unsupervised Learning)</h4> <p>Система получает <strong>неразмеченные данные</strong> и должна найти скрытые закономерности, структуру.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Неразмеченные данные:
[точка1] [точка2] [точка3] ...
[точка4] [точка5] ...

Задача: сгруппировать похожие точки
Результат: [точка1, точка3, точка5] - группа 1
           [точка2, точка4, ...] - группа 2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Основные подтипы:</strong></p> <ol><li><strong>Кластеризация</strong> — сегментация на группы (сегментация клиентов, анализ генов)</li> <li><strong>Понижение размерности</strong> — сжатие данных без потери информации (метод главных компонент)</li> <li><strong>Обучение представлений</strong> — находение скрытых признаков</li></ol> <h4 id="обучение-с-подкреплением-reinforcement-learning"><a href="#обучение-с-подкреплением-reinforcement-learning" class="header-anchor">#</a> Обучение с подкреплением (Reinforcement Learning)</h4> <p>Система учится через <strong>взаимодействие со средой</strong> и получение <strong>награды/штрафа</strong>.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Агент взаимодействует со средой:

Состояние (S) → Действие (A) → Новое состояние (S') + Награда (R)
  [позиция]   → [движение]   → [новая позиция] + [очки]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>Примеры: AlphaGo (игра в го), автономные автомобили, игры видеоигры.</p> <h2 id="модуль-3-математические-основы-неиросетеи"><a href="#модуль-3-математические-основы-неиросетеи" class="header-anchor">#</a> Модуль 3: Математические основы нейросетей</h2> <h3 id="_3-1-линеиная-алгебра-как-язык-ии"><a href="#_3-1-линеиная-алгебра-как-язык-ии" class="header-anchor">#</a> 3.1 Линейная алгебра как язык ИИ</h3> <p>Вся математика современного машинного обучения строится на четырёх концепциях линейной алгебры:[7][8][9]</p> <h4 id="скаляры"><a href="#скаляры" class="header-anchor">#</a> Скаляры</h4> <p>Просто числа. Обозначаются строчными буквами:
$$x = 5$$</p> <h4 id="векторы"><a href="#векторы" class="header-anchor">#</a> Векторы</h4> <p>Упорядоченный набор чисел (массив). Обозначаются жирным шрифтом:
$$\mathbf{x} = \begin{pmatrix} 1 \ 2 \ 3 \end{pmatrix}$$</p> <p>В ИИ вектор часто представляет <strong>один пример данных</strong>. Например, вектор для картинки — это набор значений яркости её пикселей.</p> <h4 id="матрицы"><a href="#матрицы" class="header-anchor">#</a> Матрицы</h4> <p>Двумерный массив чисел. Обозначаются большими жирными буквами:
$$\mathbf{W} = \begin{pmatrix} 1 &amp; 2 \ 3 &amp; 4 \ 5 &amp; 6 \end{pmatrix}$$</p> <p>В ИИ матрица часто представляет <strong>параметры (веса) модели</strong>. Она хранит информацию, которую модель &quot;выучила&quot;.</p> <h4 id="тензоры"><a href="#тензоры" class="header-anchor">#</a> Тензоры</h4> <p>Многомерные массивы. Например, цветная картинка размером 256×256 пикселей с 3 каналами (RGB) — это тензор размером 256×256×3.</p> <h3 id="_3-2-неирон-как-математическии-объект"><a href="#_3-2-неирон-как-математическии-объект" class="header-anchor">#</a> 3.2 Нейрон как математический объект</h3> <p><strong>Биологический нейрон</strong> — клетка в мозге, которая:[10]</p> <ul><li>Получает сигналы через <strong>дендриты</strong> (входы)</li> <li>Обрабатывает эти сигналы в <strong>теле</strong> (ядре)</li> <li>Отправляет сигнал через <strong>аксон</strong> (выход)</li></ul> <p><strong>Искусственный нейрон</strong> — математическая функция, которая имитирует этот процесс:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ВХОДЫ          ОБРАБОТКА           ВЫХОД
x₁ ──┐
     ├──→ [Σ взвешенная сумма] ──→ [f() активация] ──→ y
x₂ ──┤
x₃ ──┘

x₁ * w₁ + x₂ * w₂ + x₃ * w₃ + b = e
y = f(e)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>Формально, выход нейрона вычисляется как:
$$y = f\left(\sum_{i=1}^{n} w_i \cdot x_i + b\right) = f(\mathbf{w}^T \mathbf{x} + b)$$</p> <p>Где:</p> <ul><li>$$\mathbf{w}$$ — вектор <strong>весов</strong> (learnable parameters)</li> <li>$$\mathbf{x}$$ — вектор входов</li> <li>$$b$$ — смещение (bias, также learnable)</li> <li>$$f(\cdot)$$ — функция активации</li></ul> <h3 id="_3-3-функции-активации"><a href="#_3-3-функции-активации" class="header-anchor">#</a> 3.3 Функции активации</h3> <p><strong>Зачем нам функции активации?</strong> Без них вся нейросеть была бы просто линейной функцией (произведение матриц — это линейное преобразование). Функция активации вводит <strong>нелинейность</strong>, что позволяет сети решать сложные, нелинейные задачи.</p> <h4 id="relu-rectified-linear-unit"><a href="#relu-rectified-linear-unit" class="header-anchor">#</a> ReLU (Rectified Linear Unit)</h4> <p>$$f(x) = \max(0, x) = \begin{cases} 0 &amp; \text{если } x &lt; 0 \ x &amp; \text{если } x \geq 0 \end{cases}$$</p> <p><strong>Преимущества</strong>: быстрая для вычисления, избегает &quot;исчезновения градиента&quot;
<strong>Недостатки</strong>: мёртвые нейроны при $$x &lt; 0$$</p> <h4 id="sigmoid"><a href="#sigmoid" class="header-anchor">#</a> Sigmoid</h4> <p>$$f(x) = \frac{1}{1 + e^{-x}}$$</p> <p>Преобразует входное значение в диапазон (0, 1).</p> <p><strong>Применение</strong>: бинарная классификация (выход интерпретируется как вероятность)</p> <h4 id="tanh-гиперболическии-тангенс"><a href="#tanh-гиперболическии-тангенс" class="header-anchor">#</a> Tanh (гиперболический тангенс)</h4> <p>$$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$</p> <p>Преобразует входное значение в диапазон (-1, 1).</p> <p><strong>Применение</strong>: когда нужны и положительные, и отрицательные выходы</p> <h4 id="softmax-для-многоклассовои-классификации"><a href="#softmax-для-многоклассовои-классификации" class="header-anchor">#</a> Softmax (для многоклассовой классификации)</h4> <p>$$f(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$</p> <p>Преобразует вектор в распределение вероятностей (все значения от 0 до 1, сумма = 1).</p> <h3 id="_3-4-архитектура-многослоинои-неиросети"><a href="#_3-4-архитектура-многослоинои-неиросети" class="header-anchor">#</a> 3.4 Архитектура многослойной нейросети</h3> <p><strong>Многослойный персептрон (MLP)</strong> состоит из нескольких слоёв нейронов:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ВХОДНОЙ СЛОЙ      СКРЫТЫЕ СЛОИ        ВЫХОДНОЙ СЛОЙ
(4 нейрона)       (3 слоя × 5 нейронов)  (2 нейрона)

x₁ ●━━┓
     ┃ ╭━ ●━━┓
x₂ ●━━╋━┨     ┃ ╭━ ●
     ┃ ┃  ●━━━╋━┫     (Выход)
x₃ ●━━╋━┨     ┃ ╰━ ●
     ┃ ╰━ ●━━┛
x₄ ●━━┛      ●
             ●
             ●
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>Ключевые концепции:</strong></p> <ol><li><strong>Входной слой</strong> — это не слой в смысле нейронов, это просто входные данные</li> <li><strong>Скрытые слои</strong> — выполняют более сложную обработку</li> <li><strong>Выходной слой</strong> — даёт финальное предсказание</li> <li><strong>Полная связность</strong> — каждый нейрон одного слоя соединён со всеми нейронами следующего слоя</li></ol> <p><strong>Почему несколько слоёв?</strong> Каждый слой обнаруживает всё более сложные признаки:</p> <ul><li>Первый слой: примитивные признаки (края, текстуры)</li> <li>Второй слой: комбинации примитивов (углы, формы)</li> <li>Третий слой: сложные объекты (глаза, носы)</li> <li>Выходной слой: категория (это лицо? Какое выражение?)</li></ul> <h2 id="модуль-4-обучение-неиросетеи"><a href="#модуль-4-обучение-неиросетеи" class="header-anchor">#</a> Модуль 4: Обучение нейросетей</h2> <h3 id="_4-1-функция-потерь-loss-function"><a href="#_4-1-функция-потерь-loss-function" class="header-anchor">#</a> 4.1 Функция потерь (Loss Function)</h3> <p>Нейросеть обучается путём <strong>минимизации ошибки</strong>. Для этого нужно количественно измерить, насколько плохо сеть работает.</p> <h4 id="средняя-квадратичная-ошибка-mse"><a href="#средняя-квадратичная-ошибка-mse" class="header-anchor">#</a> Средняя квадратичная ошибка (MSE)</h4> <p>$$L = \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$</p> <p>Где:</p> <ul><li>$$y_i$$ — истинное значение</li> <li>$$\hat{y}_i$$ — предсказанное значение</li> <li>$$n$$ — количество примеров</li></ul> <p><strong>Применение</strong>: регрессия (предсказание чисел)</p> <h4 id="кросс-энтропия-cross-entropy"><a href="#кросс-энтропия-cross-entropy" class="header-anchor">#</a> Кросс-энтропия (Cross-Entropy)</h4> <p>$$L = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$</p> <p><strong>Применение</strong>: классификация (предсказание вероятностей классов)</p> <h3 id="_4-2-градиентныи-спуск"><a href="#_4-2-градиентныи-спуск" class="header-anchor">#</a> 4.2 Градиентный спуск</h3> <p>Основная идея: <strong>если мы знаем, в каком направлении функция возрастает (градиент), мы можем двигаться в противоположном направлении, чтобы найти минимум</strong>.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>      L (Loss)
       │    ╱╲
       │   ╱  ╲
       │  ╱    ╲
       │ ╱      ╲  ← Минимум (оптимальные веса)
       ├────────────► w (Вес)

Начало → Шаг 1 → Шаг 2 → Шаг 3 → ... → Минимум
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p><strong>Алгоритм:</strong></p> <ol><li>Инициализировать веса случайными значениями</li> <li>Вычислить потерю на текущих весах</li> <li>Вычислить градиент (производную) потери по каждому весу</li> <li>Обновить веса: $$w := w - \eta \cdot \frac{\partial L}{\partial w}$$</li> <li>Повторить пока потеря не стабилизируется</li></ol> <p>Где $$\eta$$ — <strong>скорость обучения (learning rate)</strong> — гиперпараметр, который контролирует величину шагов.</p> <p><strong>Выбор скорости обучения критичен:</strong></p> <ul><li>Слишком малая: обучение медленное, можем застрять в локальном минимуме</li> <li>Слишком большая: можем перепрыгнуть через минимум, обучение нестабильно</li></ul> <h3 id="_4-3-обратное-распространение-ошибки-backpropagation"><a href="#_4-3-обратное-распространение-ошибки-backpropagation" class="header-anchor">#</a> 4.3 Обратное распространение ошибки (Backpropagation)</h3> <p><strong>Ключевая проблема</strong>: как эффективно вычислить градиент потери по отношению к миллиардам весов в глубокой сети?</p> <p><strong>Ключевое решение</strong>: <strong>цепное правило</strong> из математического анализа.</p> <p>Если у нас есть составная функция:
$$L = f(g(h(x)))$$</p> <p>То:
$$\frac{dL}{dx} = \frac{dL}{df} \cdot \frac{df}{dg} \cdot \frac{dg}{dh} \cdot \frac{dh}{dx}$$</p> <p><strong>Обратное распространение</strong> применяет это правило, идя <strong>от выхода к входу</strong>, вычисляя градиент каждого слоя:[11][12][13]</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>FORWARD PASS (Прямое распространение):
Вход → Слой 1 → Слой 2 → Слой 3 → Выход
↓     (сохраняем)  (сохраняем)  (сохраняем)

BACKWARD PASS (Обратное распространение):
Выход ← Слой 3 ← Слой 2 ← Слой 1 ← Вход
(вычисляем градиент) ← ← ← ←
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Эффективность</strong>: вместо вычисления $$n$$ отдельных производных для каждого параметра, мы вычисляем их все в один проход через сеть.</p> <h3 id="_4-4-процесс-обучения-epoch-batch-iteration"><a href="#_4-4-процесс-обучения-epoch-batch-iteration" class="header-anchor">#</a> 4.4 Процесс обучения: Epoch, Batch, Iteration</h3> <p>Обучение нейросети структурируется следующим образом:[12][11]</p> <table><thead><tr><th>Термин</th> <th>Определение</th></tr></thead> <tbody><tr><td><strong>Iteration</strong></td> <td>Один шаг градиентного спуска с одним мини-батчем</td></tr> <tr><td><strong>Batch</strong></td> <td>Группа примеров, обработанные вместе</td></tr> <tr><td><strong>Epoch</strong></td> <td>Проход через все обучающие данные</td></tr></tbody></table> <p>Пример:</p> <ul><li>Обучающий набор: 1000 примеров</li> <li>Размер батча: 32 примера</li> <li>Iterations per epoch: 1000/32 ≈ 31</li> <li>После 10 epochs: общее обновление весов = 310 iterations</li></ul> <p><strong>Почему мини-батчи?</strong></p> <ol><li><strong>Вычислительная эффективность</strong> — можно параллелизировать на GPU</li> <li><strong>Шумный градиент</strong> — случайность помогает избежать локальных минимумов</li> <li><strong>Лучшее обобщение</strong> — зашумленный градиент действует как регуляризация</li></ol> <h2 id="модуль-5-проблемы-обучения-и-их-решения"><a href="#модуль-5-проблемы-обучения-и-их-решения" class="header-anchor">#</a> Модуль 5: Проблемы обучения и их решения</h2> <h3 id="_5-1-переобучение-overfitting-vs-недообучение-underfitting"><a href="#_5-1-переобучение-overfitting-vs-недообучение-underfitting" class="header-anchor">#</a> 5.1 Переобучение (Overfitting) vs Недообучение (Underfitting)</h3> <p>Это одна из наиболее важных проблем в машинном обучении:[14][15][16]</p> <h4 id="недообучение-underfitting"><a href="#недообучение-underfitting" class="header-anchor">#</a> Недообучение (Underfitting)</h4> <p>Модель <strong>слишком проста</strong> и не может захватить сложность данных.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Истинная функция: ~~волнистая кривая~~

Модель (линейная):  ______ (прямая линия)
Ошибка: ВЫСОКАЯ как на тренировке, так и на тесте
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p><strong>Решения:</strong></p> <ul><li>Увеличить сложность модели (больше слоёв, больше нейронов)</li> <li>Добавить больше признаков (feature engineering)</li> <li>Обучать дольше</li></ul> <h4 id="переобучение-overfitting"><a href="#переобучение-overfitting" class="header-anchor">#</a> Переобучение (Overfitting)</h4> <p>Модель <strong>слишком сложна</strong> и запоминает шум в данных, а не генерализирует закономерности.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Истинная функция:  ~~волнистая кривая~~
Шум:              ● ● ● (случайные точки)

Модель: ~~извилистая линия, проходящая через каждую точку~~
Ошибка: НИЗКАЯ на тренировке, но ВЫСОКАЯ на тесте
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>Это как ученик, который вызубрил все решения из учебника, но не может решить новую задачу.</p> <p><strong>Решения:</strong></p> <ol><li><p><strong>Регуляризация</strong> — добавить штраф за сложность модели:[15][14]
$$L_{\text{total}} = L_{\text{data}} + \lambda \cdot R(\mathbf{w})$$</p> <p>Где $$\lambda$$ контролирует силу регуляризации. Популярные варианты:</p> <ul><li><strong>L1</strong>: $$R = \sum |w_i|$$</li> <li><strong>L2</strong>: $$R = \sum w_i^2$$</li></ul></li> <li><p><strong>Ранняя остановка (Early Stopping)</strong> — остановить обучение когда ошибка на валидационном наборе начинает расти:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Ошибка
│
│ Обучение   Валидация
│    ╱╲         ╱
│   ╱  ╲       ╱
│  ╱    ╲    ╱ ← STOP ЗДЕСЬ
│ ╱      ╲  ╱
│╱        ╲╱
└─────────────────► Epoch
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div></li> <li><p><strong>Dropout</strong> — случайно &quot;отключать&quot; нейроны во время обучения, заставляя сеть избыточной и ненависящей от конкретных нейронов</p></li> <li><p><strong>Увеличение данных</strong> — создание новых примеров путём трансформации (для картинок: повороты, растяжения, добавление шума)</p></li></ol> <h3 id="_5-2-валидация-и-тестирование"><a href="#_5-2-валидация-и-тестирование" class="header-anchor">#</a> 5.2 Валидация и тестирование</h3> <p><strong>Никогда</strong> не оцениваем модель на тех же данных, на которых её обучали. Нужно разделение:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Все данные (100%)
│
├─ Обучающий набор (60-70%)   → используется для обучения
├─ Валидационный набор (10-15%) → используется для подбора гиперпараметров
└─ Тестовый набор (15-20%)    → используется только ОДИН РАЗ в конце
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p><strong>Кросс-валидация</strong> — улучшенный метод при малом количестве данных:</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Данные разделены на k частей (например, k=5):
Итерация 1: [Тест] [Обучение] [Обучение] [Обучение] [Обучение]
Итерация 2: [Обучение] [Тест] [Обучение] [Обучение] [Обучение]
Итерация 3: [Обучение] [Обучение] [Тест] [Обучение] [Обучение]
...
Финальная оценка = средняя ошибка по всем итерациям
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h2 id="модуль-6-современные-архитектуры-2025"><a href="#модуль-6-современные-архитектуры-2025" class="header-anchor">#</a> Модуль 6: Современные архитектуры (2025)</h2> <h3 id="_6-1-сверточные-неиросети-cnn"><a href="#_6-1-сверточные-неиросети-cnn" class="header-anchor">#</a> 6.1 Сверточные нейросети (CNN)</h3> <p><strong>Задача</strong>: распознавание паттернов в изображениях.</p> <p><strong>Ключевая идея</strong>: вместо полносвязных слоёв, использовать <strong>свёртки</strong> — скользящие окна, которые применяют один и тот же фильтр ко всему изображению.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Входное изображение:
███████
███████
███████

Фильтр (3×3):
╔═╗
║?║ (вычисляет сумму/произведение значений под ним)
╚═╝

Результат свёртки:
 ○ ○ ○ ○ ○
 ○ ○ ○ ○ ○
 ○ ○ ○ ○ ○
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p><strong>Преимущества:</strong></p> <ul><li><strong>Трансляционная инвариантность</strong> — кот в левом углу узнаётся так же хорошо, как в центре</li> <li><strong>Меньше параметров</strong> — один фильтр используется много раз</li> <li><strong>Структура данных</strong> — учитывает пространственное расположение пикселей</li></ul> <h3 id="_6-2-рекуррентные-неиросети-rnn-и-lstm"><a href="#_6-2-рекуррентные-неиросети-rnn-и-lstm" class="header-anchor">#</a> 6.2 Рекуррентные нейросети (RNN) и LSTM</h3> <p><strong>Задача</strong>: обработка последовательностей (текст, временные ряды, речь).</p> <p><strong>Ключевая идея</strong>: иметь <strong>скрытое состояние</strong> (память), которое обновляется по мере обработки каждого элемента последовательности.</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Последовательность: [word₁, word₂, word₃, ...]

h₀ (начальное состояние)
│
├─→ [RNN Cell] ─→ h₁
    (word₁)
    │
    ├─→ [RNN Cell] ─→ h₂
        (word₂)
        │
        ├─→ [RNN Cell] ─→ h₃
            (word₃)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p><strong>Проблема</strong>: RNN страдают от <strong>исчезновения градиента</strong> при длинных последовательностях.</p> <p><strong>Решение</strong>: <strong>LSTM</strong> (Long Short-Term Memory) — RNN с &quot;воротами&quot; (gates), которые контролируют, какую информацию запоминать/забывать.</p> <h3 id="_6-3-трансформеры-и-механизм-внимания"><a href="#_6-3-трансформеры-и-механизм-внимания" class="header-anchor">#</a> 6.3 Трансформеры и механизм внимания</h3> <p><strong>Задача</strong>: обработка последовательностей без рекурсии, параллельная обработка.</p> <p><strong>Ключевая инновация</strong>: <strong>механизм self-attention</strong> — каждый элемент последовательности может &quot;смотреть&quot; на все остальные элементы и взвешивать их значимость.[17][18][19]</p> <h4 id="как-работает-внимание"><a href="#как-работает-внимание" class="header-anchor">#</a> Как работает внимание</h4> <p>Для каждого слова создаются три представления:[18][19][17]</p> <ul><li><strong>Query (Q)</strong> — &quot;Что я ищу?&quot;</li> <li><strong>Key (K)</strong> — &quot;Как меня можно описать?&quot;</li> <li><strong>Value (V)</strong> — &quot;Какую информацию я несу?&quot;</li></ul> <p>Процесс:</p> <ol><li><p>Для каждой пары слов вычисляем <strong>релевантность</strong>:
$$\text{score}(q, k) = \frac{\mathbf{q} \cdot \mathbf{k}}{\sqrt{d_k}}$$</p></li> <li><p>Нормализуем через softmax, чтобы получить <strong>внимательные веса</strong>:
$$\alpha = \text{softmax}(\text{score})$$</p></li> <li><p>Взвешиваем значения:
$$\text{output} = \sum \alpha \cdot \mathbf{v}$$</p></li></ol> <div class="language- line-numbers-mode"><pre class="language-text"><code>Пример: &quot;The cat sat on the mat&quot;

Слово &quot;sat&quot; может уделить:
- 70% внимания &quot;cat&quot; (подлежащее)
- 20% внимания &quot;on&quot;  (предлог)
- 5% внимания &quot;mat&quot;  (объект)
- 5% внимания остальным

Результат: &quot;sat&quot; получает информацию о том, что кот что-то делает
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p><strong>Почему это революционно:</strong></p> <ul><li><strong>Параллелизм</strong>: можно обработать всю последовательность за раз (в отличие от RNN, которые работают последовательно)</li> <li><strong>Долгосрочные зависимости</strong>: слова далеко друг от друга могут напрямую влиять друг на друга</li> <li><strong>Интерпретируемость</strong>: можно увидеть, на что &quot;смотрит&quot; модель</li></ul> <p><strong>Трансформер полностью состоит из</strong>:</p> <ol><li>Несколько слоёв self-attention (Multi-Head Attention)</li> <li>Feed-forward сети (MLP)</li> <li>Нормализация и резидуальные связи</li></ol> <h3 id="_6-4-большие-языковые-модели-llm"><a href="#_6-4-большие-языковые-модели-llm" class="header-anchor">#</a> 6.4 Большие языковые модели (LLM)</h3> <p><strong>GPT, Claude, Gemini и т.д.</strong> — это по сути <strong>трансформеры, обученные на триллионах токенов текста</strong>.</p> <p>Они предсказывают &quot;<strong>следующее слово в последовательности</strong>&quot;:
$$P(\text{word}_t | \text{word}_0, \text{word}<em>1, ..., \text{word}</em>{t-1})$$</p> <p>Параллель с аналогией попугая:</p> <ul><li>Попугай видел миллионы диалогов и выучил закономерности</li> <li>LLM видел триллионы слов в интернете и выучил закономерности языка</li> <li>Когда вы даёте подсказку, модель генерирует ответ, продолжая эти закономерности</li></ul> <h2 id="модуль-7-актуальное-состояние-ии-в-2025-году"><a href="#модуль-7-актуальное-состояние-ии-в-2025-году" class="header-anchor">#</a> Модуль 7: Актуальное состояние ИИ в 2025 году</h2> <h3 id="_7-1-основные-направления-исследовании"><a href="#_7-1-основные-направления-исследовании" class="header-anchor">#</a> 7.1 Основные направления исследований</h3> <p><strong>Мультимодальность</strong>: модели могут работать с текстом, изображениями, аудио, видео одновременно</p> <ul><li>GPT-4 Vision может описывать изображения</li> <li>Gemini обрабатывает видео</li> <li>Sora генерирует видео из текста</li></ul> <p><strong>Эффективность</strong>: более маленькие, быстрые модели</p> <ul><li>Distillation — создание маленьких моделей из больших</li> <li>Quantization — сжатие весов (32-bit → 8-bit)</li> <li>Sparse модели — отключение ненужных параметров</li></ul> <p><strong>Специализация</strong>: вместо одной универсальной модели, множество специализированных</p> <ul><li>Med-PaLM для медицины</li> <li>AlphaFold для биология</li> <li>Codex для программирования</li></ul> <p><strong>Open Source</strong>: демократизация ИИ</p> <ul><li>Llama от Meta — достаточно хорошая для большинства задач</li> <li>Mistral — европейская эффективная модель</li> <li>LocalAI — локальный запуск ИИ</li></ul> <h3 id="_7-2-остающиеся-неизвестные"><a href="#_7-2-остающиеся-неизвестные" class="header-anchor">#</a> 7.2 Остающиеся неизвестные</h3> <p><strong>Эмерджентные способности</strong>: большие модели внезапно начинают делать то, на что они не обучались явно</p> <ul><li>Zero-shot learning: решение задач без единого примера</li> <li>Few-shot learning: решение после одного-двух примеров</li></ul> <p><strong>Экспоненциальное масштабирование</strong>: становится ли интеллект экспоненциальным с размером модели?</p> <p><strong>AGI (Artificial General Intelligence)</strong>: сможет ли ИИ делать ВСЕ, что могут люди?</p> <ul><li>OpenAI думает, что в течение этого десятилетия</li> <li>Другие учёные скептичны</li></ul> <h3 id="_7-3-критические-вопросы"><a href="#_7-3-критические-вопросы" class="header-anchor">#</a> 7.3 Критические вопросы</h3> <p><strong>Безопасность</strong>: как гарантировать, что ИИ действует в соответствии с нашими намерениями?</p> <p><strong>Смещение (Bias)</strong>: если ИИ обучен на предвзятых данных, он будет дискриминировать</p> <p><strong>Окружающая среда</strong>: обучение большой модели требует столько же электричества, сколько небольшой город за месяц</p> <p><strong>Экономика</strong>: какие работы исчезнут? Кто получит выгоду от ИИ?</p> <h2 id="модуль-8-практическое-применение-и-развитие-навыков"><a href="#модуль-8-практическое-применение-и-развитие-навыков" class="header-anchor">#</a> Модуль 8: Практическое применение и развитие навыков</h2> <h3 id="_8-1-где-начать-обучение"><a href="#_8-1-где-начать-обучение" class="header-anchor">#</a> 8.1 Где начать обучение</h3> <p><strong>Уровень 0 (Текущий)</strong>: понимание концепций</p> <ul><li>✓ Вы уже это делаете</li></ul> <p><strong>Уровень 1</strong>: реализация простых моделей</p> <ul><li>PyTorch/TensorFlow tutorial</li> <li>Классификация MNIST</li> <li>Линейная регрессия</li></ul> <p><strong>Уровень 2</strong>: работа с реальными данными</p> <ul><li>Kaggle competitions</li> <li>Практические проекты</li> <li>Transfer learning</li></ul> <p><strong>Уровень 3</strong>: специализация</p> <ul><li>NLP, Computer Vision, RL</li> <li>Исследование новых архитектур</li> <li>Контрибьюция в open source</li></ul> <h3 id="_8-2-инструменты-и-фреимворки"><a href="#_8-2-инструменты-и-фреимворки" class="header-anchor">#</a> 8.2 Инструменты и фреймворки</h3> <table><thead><tr><th>Инструмент</th> <th>Применение</th> <th>Стиль</th></tr></thead> <tbody><tr><td><strong>PyTorch</strong></td> <td>Исследование, научная работа</td> <td>Pythonic, динамичный</td></tr> <tr><td><strong>TensorFlow/Keras</strong></td> <td>Продакшен, production</td> <td>Структурированный</td></tr> <tr><td><strong>JAX</strong></td> <td>Исследование, высокопроизводительные вычисления</td> <td>Функциональный</td></tr> <tr><td><strong>scikit-learn</strong></td> <td>Классический ML</td> <td>Простой, ясный</td></tr> <tr><td><strong>Hugging Face</strong></td> <td>NLP, готовые модели</td> <td>User-friendly</td></tr></tbody></table> <h3 id="_8-3-этика-и-ответственность"><a href="#_8-3-этика-и-ответственность" class="header-anchor">#</a> 8.3 Этика и ответственность</h3> <p>Как специалист по ИИ, вы должны помнить:</p> <ol><li><strong>Данные имеют значение</strong> — чистите данные, проверяйте на смещение</li> <li><strong>Прозрачность</strong> — объясняйте, как работает ваша модель</li> <li><strong>Ответственность</strong> — думайте о возможных вредных последствиях</li> <li><strong>Честность</strong> — не переоцениваете возможности, не скрывайте ограничения</li></ol> <h2 id="заключение-куда-дальше"><a href="#заключение-куда-дальше" class="header-anchor">#</a> Заключение: Куда дальше?</h2> <p>ИИ в 2025 году находится в точке <strong>максимума ожиданий</strong> (пик цикла Гартнера). Многое из того, что обещают, действительно возможно, но есть и серьёзные ограничения.</p> <p><strong>Помните:</strong></p> <ul><li>ИИ <strong>не думает и не понимает</strong> — это математика, статистика и поиск паттернов</li> <li>ИИ <strong>не магичен</strong> — он работает благодаря миллиардам параметров, обученных на триллионах примеров</li> <li>ИИ <strong>полезен, но не всемогущ</strong> — есть задачи, которые ИИ решает хорошо, а есть задачи, где люди незаменимы</li> <li>ИИ <strong>отражает данные</strong> — если данные предвзяты, выхода будет предвзятым</li></ul> <p><strong>Ваша роль как студента и будущего профессионала:</strong></p> <ol><li><strong>Понимать</strong> — как это работает под капотом</li> <li><strong>Критиковать</strong> — задавать вопросы, ставить под сомнение предположения</li> <li><strong>Развивать</strong> — создавать инструменты, которые помогают людям</li> <li><strong>Защищать</strong> — думать о последствиях и ответственности</li></ol> <p>Искусственный интеллект — это не просто технология. Это отражение нашего понимания интеллекта, сознания и того, что значит быть человеком. Изучая ИИ, вы изучаете самих себя.</p> <hr> <h2 id="рекомендации-для-дальнеишего-изучения"><a href="#рекомендации-для-дальнеишего-изучения" class="header-anchor">#</a> Рекомендации для дальнейшего изучения</h2> <h3 id="научные-работы-классические"><a href="#научные-работы-классические" class="header-anchor">#</a> Научные работы (классические)</h3> <ul><li><strong>Tuthorial Turing (1950)</strong>: &quot;Computing Machinery and Intelligence&quot;</li> <li><strong>Werbos (1974)</strong>: Обратное распространение в нейросетях</li> <li><strong>LeCun et al. (2015)</strong>: &quot;Deep Learning&quot; — обзорная статья в Nature</li> <li><strong>Vaswani et al. (2017)</strong>: &quot;Attention is All You Need&quot; — статья о трансформерах</li></ul> <h3 id="учебники-и-курсы"><a href="#учебники-и-курсы" class="header-anchor">#</a> Учебники и курсы</h3> <ul><li><strong>Andrew Ng's Machine Learning Specialization</strong> (Coursera)</li> <li><strong>Stanford CS224N</strong>: NLP with Deep Learning</li> <li><strong>MIT 6.S191</strong>: Introduction to Deep Learning</li> <li><strong>Fast.ai</strong>: Practical Deep Learning</li></ul> <h3 id="практическое-применение"><a href="#практическое-применение" class="header-anchor">#</a> Практическое применение</h3> <ul><li><strong>Kaggle</strong>: соревнования по ML</li> <li><strong>Papers with Code</strong>: реализация последних исследований</li> <li><strong>Hugging Face</strong>: готовые модели и датасеты</li></ul> <hr> <p><strong>Конец лекции. Вопросы?</strong></p> <h2 id="сноски"><a href="#сноски" class="header-anchor">#</a> Сноски</h2> <p><a href="https://habr.com/ru/companies/intec_balance/articles/864232/" target="_blank" rel="noopener noreferrer">1<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://adpass.ru/istoriya-razvitiya-iskusstvennogo-intellekta/" target="_blank" rel="noopener noreferrer">2<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://chelib.ru/articles/iskusstvennyj-intellekt-istorija-razvitija-perspektivy-v-budushhem/" target="_blank" rel="noopener noreferrer">3<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://jaycopilot.com/blog/istoriya-razvitiya-iskusstvennogo-intellekta-podrobnyj-analiz-evolyutsii-i-perspektiv" target="_blank" rel="noopener noreferrer">4<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://www.hse.ru/edu/courses/1048793063" target="_blank" rel="noopener noreferrer">5<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://iphras.ru/uplfile/root/projects/cogn_senter/3.pdf" target="_blank" rel="noopener noreferrer">6<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://otus.ru/nest/post/1526/" target="_blank" rel="noopener noreferrer">7<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://www.fa.ru/university/structure/university/uso/press-service/press-releases/iskusstvennyy-intellekt-i-lineynaya-algebra-vzglyad-cherez-dvoystvennoe-prostranstvo-" target="_blank" rel="noopener noreferrer">8<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://tproger.ru/translations/math-for-ai-linear-algebra" target="_blank" rel="noopener noreferrer">9<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://www.arcsinus.ru/blog/neuronet-architecture" target="_blank" rel="noopener noreferrer">10<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://sky.pro/wiki/python/obuchenie-nejronnoj-seti-s-obratnym-rasprostraneniem-oshibki/" target="_blank" rel="noopener noreferrer">11<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://robocraft.ru/algorithm/560" target="_blank" rel="noopener noreferrer">12<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki" target="_blank" rel="noopener noreferrer">13<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://habr.com/ru/companies/surfingbird/articles/143455/" target="_blank" rel="noopener noreferrer">14<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://codelabsacademy.com/ru/%D0%B1%D0%BB%D0%BE%D0%B3/overfitting-and-underfitting-in-machine-learning" target="_blank" rel="noopener noreferrer">15<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://codelabsacademy.com/ru/blog/overfitting-and-underfitting-in-machine-learning" target="_blank" rel="noopener noreferrer">16<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://serverflow.ru/blog/stati/epokha-ii-transformerov-chto-eto-za-arkhitektura-i-kak-rabotaet-mekhanizm-vnimaniya/" target="_blank" rel="noopener noreferrer">17<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://dtf.ru/id2687299/3861067-vnimanie-v-ii-kak-rabotayet-i-pochemu-vazhno" target="_blank" rel="noopener noreferrer">18<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://education.yandex.ru/handbook/ml/article/transformery" target="_blank" rel="noopener noreferrer">19<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://journals.rudn.ru/philosophy/article/view/42157/ru_RU" target="_blank" rel="noopener noreferrer">20<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener noreferrer">21<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Последнее обновление:</span> <span class="time">11/19/2025, 11:30:41 AM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/aikb/topics/local-ai-models.html" class="prev">
        Локальные нейросети: зачем ставить на компьютер и как
      </a></span> <span class="next"><a href="/aikb/topics/embeddings.html">
        Эмбеддинги: как ИИ понимает смысл
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"><!----></div></div>
    <script src="/aikb/assets/js/app.7ae53dfc.js" defer></script><script src="/aikb/assets/js/2.7772ee18.js" defer></script><script src="/aikb/assets/js/1.64231b73.js" defer></script><script src="/aikb/assets/js/40.9f2b4cd6.js" defer></script>
  </body>
</html>

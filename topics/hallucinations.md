# Почему ИИ «галлюцинирует»

**Галлюцинация** (hallucination) — это когда ИИ придумывает информацию, которой на самом деле нет, или выдаёт её за правду.

Название «галлюцинация» используется потому, что ИИ «видит» или «помнит» то, чего на самом деле не существует — похоже на человеческие галлюцинации.

## Почему это происходит?

ИИ не хранит факты как база данных. Вместо этого он генерирует ответы, угадывая, какие слова должны идти дальше, основываясь на том, что видел при обучении.

Иногда это угадывание приводит к тому, что ИИ:
- Придумывает несуществующие факты
- Выдаёт выдуманные цитаты за реальные
- Создаёт несуществующие книги, фильмы, события
- Путает даты и имена

## Примеры галлюцинаций

- ИИ может назвать несуществующую книгу и рассказать про её содержание
- Может придумать историческое событие с точными датами
- Может «процитировать» высказывание, которого никогда не было
- Может назвать неправильную дату известного события

## Как распознать галлюцинацию?

⚠️ ИИ очень уверенно говорит о фактах, которые легко проверить
⚠️ Даёт очень подробные детали о чём-то, что сложно проверить
⚠️ «Цитирует» источники, которых не существует
⚠️ Путает похожие факты или имена

## Что делать?

1. **Всегда проверяй важные факты** в других источниках
2. **Проси ИИ указать источники** — если он не может, это подозрительно
3. **Задавай уточняющие вопросы** — «Откуда ты это знаешь?», «Ты уверен?»
4. **Используй ИИ для идей, а не для фактов** — для фактов лучше поисковик

## Важно понимать

Галлюцинации — это не баг, а особенность того, как работают языковые модели. Они генерируют правдоподобные ответы, но не проверяют их на истинность.

## Уровень
1

## Примеры из жизни
- ИИ может придумать несуществующую научную статью
- Может назвать неправильную дату исторического события
- Может «процитировать» высказывание знаменитости, которого не было

## Полезные ссылки для этого уровня
- [Проблема галлюцинаций в ИИ](ссылка)

## Заметки
